import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import nibabel as nib
from torch.utils.data import Dataset, DataLoader
import os
from pathlib import Path
import sys
from collections import OrderedDict
from copy import deepcopy
from typing import Union, Tuple, List
from scipy.ndimage import map_coordinates
from skimage.transform import resize
import pandas as pd

# å°å…¥ä½ çš„æ¨¡çµ„ï¼ˆæ ¹æ“šå¯¦éš›è·¯å¾‘èª¿æ•´ï¼‰
sys.path.append(r"D:\unet3d")
from .augmentation_function import *
import warnings
warnings.filterwarnings('ignore')

# ==================== nnUNet é¢¨æ ¼çš„ Resampling å‡½æ•¸ ====================
ANISO_THRESHOLD = 3  # å„å‘ç•°æ€§é–¾å€¼

def get_do_separate_z(spacing: Union[Tuple[float, ...], List[float], np.ndarray], 
                      anisotropy_threshold=ANISO_THRESHOLD):
    """åˆ¤æ–·æ˜¯å¦éœ€è¦åˆ†é›¢è™•ç† z è»¸"""
    do_separate_z = (np.max(spacing) / np.min(spacing)) > anisotropy_threshold
    return do_separate_z


def get_lowres_axis(spacing: Union[Tuple[float, ...], List[float], np.ndarray]):
    """æ‰¾å‡ºå„å‘ç•°æ€§çš„è»¸"""
    axis = np.where(max(spacing) / np.array(spacing) == 1)[0]
    return axis


def compute_new_shape(old_shape: Union[Tuple[int, ...], List[int], np.ndarray],
                      old_spacing: Union[Tuple[float, ...], List[float], np.ndarray],
                      new_spacing: Union[Tuple[float, ...], List[float], np.ndarray]) -> np.ndarray:
    """æ ¹æ“š spacing è¨ˆç®—æ–°çš„å½¢ç‹€"""
    assert len(old_spacing) == len(old_shape)
    assert len(old_shape) == len(new_spacing)
    new_shape = np.array([int(round(i / j * k)) for i, j, k in zip(old_spacing, new_spacing, old_shape)])
    return new_shape


def determine_do_sep_z_and_axis(
        force_separate_z: bool,
        current_spacing,
        new_spacing,
        separate_z_anisotropy_threshold: float = ANISO_THRESHOLD) -> Tuple[bool, Union[int, None]]:
    """æ±ºå®šæ˜¯å¦éœ€è¦åˆ†é›¢ z è»¸è™•ç†ä»¥åŠå“ªå€‹è»¸"""
    if force_separate_z is not None:
        do_separate_z = force_separate_z
        if force_separate_z:
            axis = get_lowres_axis(current_spacing)
        else:
            axis = None
    else:
        if get_do_separate_z(current_spacing, separate_z_anisotropy_threshold):
            do_separate_z = True
            axis = get_lowres_axis(current_spacing)
        elif get_do_separate_z(new_spacing, separate_z_anisotropy_threshold):
            do_separate_z = True
            axis = get_lowres_axis(new_spacing)
        else:
            do_separate_z = False
            axis = None

    if axis is not None:
        if len(axis) == 3:
            do_separate_z = False
            axis = None
        elif len(axis) == 2:
            do_separate_z = False
            axis = None
        else:
            axis = axis[0]
    return do_separate_z, axis


def resample_data_or_seg(data: np.ndarray, 
                         new_shape: Union[Tuple[float, ...], List[float], np.ndarray],
                         is_seg: bool = False, 
                         axis: Union[None, int] = None, 
                         order: int = 3,
                         do_separate_z: bool = False, 
                         order_z: int = 0, 
                         dtype_out=None):
    """
    nnUNet é¢¨æ ¼çš„é‡æ¡æ¨£å‡½æ•¸
    
    Args:
        data: è¼¸å…¥è³‡æ–™ (c, x, y, z) æˆ– (x, y, z)
        new_shape: ç›®æ¨™å½¢ç‹€
        is_seg: æ˜¯å¦ç‚ºåˆ†å‰²æ¨™ç±¤
        axis: å„å‘ç•°æ€§è»¸
        order: æ’å€¼éšæ•¸ï¼ˆå½±åƒï¼‰
        do_separate_z: æ˜¯å¦åˆ†é›¢è™•ç† z è»¸
        order_z: z è»¸æ’å€¼éšæ•¸
        dtype_out: è¼¸å‡ºè³‡æ–™å‹åˆ¥
    """
    # ç¢ºä¿è³‡æ–™æ˜¯ 4D (c, x, y, z)
    if data.ndim == 3:
        data = data[np.newaxis, ...]
        squeeze_output = True
    else:
        squeeze_output = False
    
    assert data.ndim == 4, "data must be (c, x, y, z)"
    assert len(new_shape) == data.ndim - 1

    if is_seg:
        # å°æ–¼åˆ†å‰²æ¨™ç±¤ï¼Œä½¿ç”¨æœ€è¿‘é„°æ’å€¼
        resize_fn = lambda img, shape, order, **kwargs: resize(img, shape, order=0, preserve_range=True, anti_aliasing=False)
        kwargs = {}
    else:
        resize_fn = resize
        kwargs = {'mode': 'edge', 'anti_aliasing': False}
    
    shape = np.array(data[0].shape)
    new_shape = np.array(new_shape)
    
    if dtype_out is None:
        dtype_out = data.dtype
    
    reshaped_final = np.zeros((data.shape[0], *new_shape), dtype=dtype_out)
    
    if np.any(shape != new_shape):
        data = data.astype(float, copy=False)
        
        if do_separate_z:
            assert axis is not None, 'If do_separate_z, we need to know what axis is anisotropic'
            
            # æ±ºå®š 2D å¹³é¢çš„å½¢ç‹€
            if axis == 0:
                new_shape_2d = new_shape[1:]
            elif axis == 1:
                new_shape_2d = new_shape[[0, 2]]
            else:
                new_shape_2d = new_shape[:-1]

            for c in range(data.shape[0]):
                tmp = deepcopy(new_shape)
                tmp[axis] = shape[axis]
                reshaped_here = np.zeros(tmp)
                
                # å…ˆåœ¨ 2D å¹³é¢ä¸Š resize
                for slice_id in range(shape[axis]):
                    if axis == 0:
                        reshaped_here[slice_id] = resize_fn(data[c, slice_id], new_shape_2d, order, **kwargs)
                    elif axis == 1:
                        reshaped_here[:, slice_id] = resize_fn(data[c, :, slice_id], new_shape_2d, order, **kwargs)
                    else:
                        reshaped_here[:, :, slice_id] = resize_fn(data[c, :, :, slice_id], new_shape_2d, order, **kwargs)
                
                # ç„¶å¾Œå–®ç¨è™•ç† z è»¸
                if shape[axis] != new_shape[axis]:
                    rows, cols, dim = new_shape[0], new_shape[1], new_shape[2]
                    orig_rows, orig_cols, orig_dim = reshaped_here.shape

                    # align_corners=False
                    row_scale = float(orig_rows) / rows
                    col_scale = float(orig_cols) / cols
                    dim_scale = float(orig_dim) / dim

                    map_rows, map_cols, map_dims = np.mgrid[:rows, :cols, :dim]
                    map_rows = row_scale * (map_rows + 0.5) - 0.5
                    map_cols = col_scale * (map_cols + 0.5) - 0.5
                    map_dims = dim_scale * (map_dims + 0.5) - 0.5

                    coord_map = np.array([map_rows, map_cols, map_dims])
                    
                    if not is_seg or order_z == 0:
                        reshaped_final[c] = map_coordinates(reshaped_here, coord_map, order=order_z, mode='nearest')
                    else:
                        # å°æ–¼åˆ†å‰²æ¨™ç±¤ï¼Œä½¿ç”¨å¤šæ•¸æŠ•ç¥¨
                        unique_labels = np.sort(pd.unique(reshaped_here.ravel()))
                        for i, cl in enumerate(unique_labels):
                            reshaped_final[c][np.round(
                                map_coordinates((reshaped_here == cl).astype(float), coord_map, order=order_z,
                                                mode='nearest')) > 0.5] = cl
                else:
                    reshaped_final[c] = reshaped_here
        else:
            # ä¸åˆ†é›¢ z è»¸ï¼Œç›´æ¥ 3D resize
            for c in range(data.shape[0]):
                reshaped_final[c] = resize_fn(data[c], new_shape, order, **kwargs)
        
        if squeeze_output:
            reshaped_final = reshaped_final[0]
        
        return reshaped_final
    else:
        if squeeze_output:
            return data[0]
        return data


# ==================== æ”¹è‰¯ç‰ˆ NII.GZ æª”æ¡ˆè®€å– Dataset ====================
class MedicalImageDataset(Dataset):
    """
    é†«å­¸å½±åƒ NII.GZ æª”æ¡ˆè®€å– Dataset (ä½¿ç”¨ nnUNet é¢¨æ ¼çš„ resampling)
    """
    def __init__(self, data_root, split='train', image_suffix=['.nii.gz', '.nii'], 
                 mask_suffix=['.nii.gz', '.nii'], transform=None, target_size=None, 
                 num_classes=None, debug_labels=True, use_augmentation=True,
                 augmentation_type='medical', spacing=None, force_separate_z=None,
                ):
        """
        Args:
            data_root: è³‡æ–™æ ¹ç›®éŒ„
            split: è³‡æ–™åˆ†å‰² ('train', 'val', 'test')
            image_suffix: å½±åƒæª”æ¡ˆå¾Œç¶´
            mask_suffix: æ¨™ç±¤æª”æ¡ˆå¾Œç¶´
            transform: é¡å¤–çš„è³‡æ–™å¢å¼·è®Šæ›
            target_size: ç›®æ¨™å°ºå¯¸ (D, H, W)
            num_classes: é¡åˆ¥æ•¸é‡
            debug_labels: æ˜¯å¦å•Ÿç”¨æ¨™ç±¤é™¤éŒ¯
            use_augmentation: æ˜¯å¦ä½¿ç”¨æ•¸æ“šå¢å¼·
            augmentation_type: æ•¸æ“šå¢å¼·é¡å‹
            spacing: åŸå§‹è³‡æ–™çš„ spacing (z, y, x)ï¼Œç”¨æ–¼åˆ¤æ–·å„å‘ç•°æ€§
            force_separate_z: å¼·åˆ¶æ˜¯å¦åˆ†é›¢ z è»¸è™•ç†
        """
        self.data_root = Path(data_root)
        self.split = split
        self.external_transform = transform
        self.target_size = target_size
        self.num_classes = num_classes
        self.debug_labels = debug_labels
        self.spacing = spacing if spacing is not None else [1.0, 1.0, 1.0]  # é è¨­ç­‰å‘æ€§
        self.force_separate_z = force_separate_z

        
        
        # æ•¸æ“šå¢å¼·è¨­å®š
        self.use_augmentation = use_augmentation and (split == 'train')
        if self.use_augmentation:
            if augmentation_type == 'light':
                self.augmentation_transform = get_light_augmentation()
            elif augmentation_type == 'medium':
                self.augmentation_transform = get_medium_augmentation()
            elif augmentation_type == 'heavy':
                self.augmentation_transform = get_heavy_augmentation()
            elif augmentation_type == 'medical':
                self.augmentation_transform = get_medical_augmentation()
            elif augmentation_type == 'medical_heavy':
                self.augmentation_transform = get_medical_artifact_heavy()
            elif augmentation_type == 'custom':
                self.augmentation_transform = get_custom_augmentation()
            else:
                print(f"âš ï¸ æœªçŸ¥çš„å¢å¼·é¡å‹: {augmentation_type}, ä½¿ç”¨é è¨­ medical")
                self.augmentation_transform = get_medical_augmentation()
            
            print(f"âœ… è¨“ç·´é›†å•Ÿç”¨ {augmentation_type} æ•¸æ“šå¢å¼· (nnUNet é¢¨æ ¼ resampling)")
        else:
            self.augmentation_transform = None
            if split == 'train':
                print("âŒ è¨“ç·´é›†æœªå•Ÿç”¨æ•¸æ“šå¢å¼·")
            else:
                print(f"âœ… {split.upper()} é›†æœªå•Ÿç”¨æ•¸æ“šå¢å¼·ï¼ˆæ­£ç¢ºï¼‰")
        
        # è¨­å®šå½±åƒå’Œæ¨™ç±¤ç›®éŒ„
        self.image_dir = self.data_root / split / 'images'
        self.mask_dir = self.data_root / split / 'labels'
        
        # æª¢æŸ¥ç›®éŒ„
        if not self.image_dir.exists():
            raise FileNotFoundError(f"å½±åƒç›®éŒ„ä¸å­˜åœ¨: {self.image_dir}")
        if not self.mask_dir.exists():
            raise FileNotFoundError(f"æ¨™ç±¤ç›®éŒ„ä¸å­˜åœ¨: {self.mask_dir}")
        
        # ç¢ºä¿ suffix æ˜¯åˆ—è¡¨
        if isinstance(image_suffix, str):
            image_suffix = [image_suffix]
        if isinstance(mask_suffix, str):
            mask_suffix = [mask_suffix]
        
        # å°‹æ‰¾é…å°çš„æª”æ¡ˆ
        self.image_files = []
        for suffix in image_suffix:
            self.image_files.extend(list(self.image_dir.glob(f'*{suffix}')))
        
        self.image_files = list(set(self.image_files))
        self.pairs = []
        
        for img_file in self.image_files:
            img_stem = img_file.stem
            if img_stem.endswith('.nii'):
                img_stem = img_stem[:-4]
            
            mask_found = False
            for suffix in mask_suffix:
                mask_file = self.mask_dir / f"{img_stem}{suffix}"
                if mask_file.exists():
                    self.pairs.append((img_file, mask_file))
                    mask_found = True
                    break
            
            if not mask_found:
                print(f"âš ï¸ æ‰¾ä¸åˆ°å°æ‡‰çš„æ¨™ç±¤æª”æ¡ˆ {img_stem}")
        
        print(f"ğŸ“Š [{split.upper()}] æ‰¾åˆ° {len(self.pairs)} å°è³‡æ–™")
        
        if self.debug_labels and len(self.pairs) > 0:
            self.analyze_label_distribution()
    
    def analyze_label_distribution(self):
        """åˆ†ææ¨™ç±¤åˆ†ä½ˆ"""
        all_unique_values = set()
        sample_size = min(5, len(self.pairs))
        
        for i in range(sample_size):
            _, mask_path = self.pairs[i]
            try:
                mask = self.load_nii_image(mask_path)
                unique_vals = np.unique(mask)
                all_unique_values.update(unique_vals)
            except Exception as e:
                print(f"âŒ åˆ†ææª”æ¡ˆ {mask_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        print(f"ğŸ” æ¨™ç±¤å€¼ç¯„åœ: {sorted(list(all_unique_values))}")
    
    def __len__(self):
        return len(self.pairs)
    
    def load_nii_image(self, file_path):
        """è¼‰å…¥ NII.GZ æª”æ¡ˆä¸¦æå– spacing"""
        try:
            nii_img = nib.load(str(file_path))
            img_data = nii_img.get_fdata()
            img_data = np.array(img_data, dtype=np.float32)
            
            # æå– spacing è³‡è¨Šï¼ˆå¦‚æœé‚„æ²’è¨­å®šï¼‰
            if self.spacing is None or np.allclose(self.spacing, [1.0, 1.0, 1.0]):
                file_spacing = nii_img.header.get_zooms()[:3]  # (x, y, z) æˆ– (z, y, x)
                # è½‰æ›ç‚º (z, y, x) é †åº
                self.spacing = [float(file_spacing[2]), float(file_spacing[1]), float(file_spacing[0])]
                # if self.debug_labels:
                #     print(f"ğŸ“ å¾æª”æ¡ˆæå– spacing: {self.spacing} (z, y, x)")
            
            if img_data.ndim == 4:
                img_data = img_data[:, :, :, 0]
            elif img_data.ndim < 3:
                raise ValueError(f"åœ–åƒç¶­åº¦éä½: {img_data.ndim}D")
            elif img_data.ndim > 4:
                raise ValueError(f"ä¸æ”¯æ´çš„åœ–åƒç¶­åº¦: {img_data.ndim}D")
            
            return img_data
        except Exception as e:
            print(f"âŒ è®€å–æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise
    
    def normalize_image(self, image):
        """å½±åƒæ¨™æº–åŒ–"""
        p1, p99 = np.percentile(image, (1, 99))
        image = np.clip(image, p1, p99)
        
        mean = np.mean(image)
        std = np.std(image)
        if std > 0:
            image = (image - mean) / std
        
        return image
    
    def resize_volume(self, volume, target_size, is_seg=False):
        """
        ä½¿ç”¨ nnUNet é¢¨æ ¼çš„ resampling
        æ”¯æ´å„å‘ç•°æ€§è™•ç†
        """
        if target_size is None:
            return volume
        
        # ç¢ºä¿è¼¸å…¥æ˜¯ 3D
        if volume.ndim != 3:
            raise ValueError(f"resize_volume åªæ”¯æ´3Dé«”ç©ï¼Œæ”¶åˆ° {volume.ndim}D")
        
        current_size = volume.shape
        
        if len(target_size) != 3:
            raise ValueError(f"target_size å¿…é ˆæ˜¯3D (D, H, W)")
        
        # è¨ˆç®—æ–°çš„ spacing
        current_spacing = np.array(self.spacing)
        new_spacing = current_spacing * (np.array(current_size) / np.array(target_size))
        
        # æ±ºå®šæ˜¯å¦éœ€è¦åˆ†é›¢ z è»¸
        do_separate_z, axis = determine_do_sep_z_and_axis(
            self.force_separate_z, 
            current_spacing, 
            new_spacing
        )
        
        # ä½¿ç”¨ nnUNet é¢¨æ ¼çš„ resampling
        order = 0 if is_seg else 3  # æ¨™ç±¤ç”¨æœ€è¿‘é„°ï¼Œå½±åƒç”¨ä¸‰æ¬¡æ’å€¼
        order_z = 0  # z è»¸é€šå¸¸ä½¿ç”¨æœ€è¿‘é„°æˆ–ç·šæ€§æ’å€¼
        
        resized = resample_data_or_seg(
            volume,
            target_size,
            is_seg=is_seg,
            axis=axis,
            order=order,
            do_separate_z=do_separate_z,
            order_z=order_z
        )
        
        return resized
    
    def clean_labels(self, mask, file_path=None):
        """æ¸…ç†æ¨™ç±¤"""
        if mask.ndim != 3:
            if mask.ndim == 4:
                mask = mask[:, :, :, 0]
        
        mask = np.nan_to_num(mask, nan=0.0, posinf=0.0, neginf=0.0)
        mask[mask > 0] = 1
        mask = mask.astype(np.int64)
        
        unique_vals = np.unique(mask)
        if not set(unique_vals).issubset({0, 1}):
            print(f"âš ï¸ ç™¼ç¾ç•°å¸¸æ¨™ç±¤å€¼: {unique_vals}")
            mask = (mask > 0).astype(np.int64)
        
        return mask
    
    def handle_dimension_mismatch(self, image, mask, file_path=None):
        """è™•ç†ç¶­åº¦ä¸åŒ¹é…"""
        if image.shape != mask.shape:
            if image.ndim == 4 and mask.ndim == 3:
                image = image[:, :, :, 0]
            elif image.ndim == 3 and mask.ndim == 4:
                mask = mask[:, :, :, 0]
            
            if image.shape != mask.shape:
                min_shape = [min(i, m) for i, m in zip(image.shape, mask.shape)]
                image = image[:min_shape[0], :min_shape[1], :min_shape[2]]
                mask = mask[:min_shape[0], :min_shape[1], :min_shape[2]]
        
        return image, mask
    
    def __getitem__(self, idx):
        img_path, mask_path = self.pairs[idx]
        
        try:
            # è¼‰å…¥å½±åƒå’Œæ¨™ç±¤
            image = self.load_nii_image(img_path)
            mask = self.load_nii_image(mask_path)
            
            # è™•ç†ç¶­åº¦ä¸åŒ¹é…
            image, mask = self.handle_dimension_mismatch(image, mask, img_path.name)
            

            image = self.resize_volume(image, self.target_size, is_seg=False)
            mask = self.resize_volume(mask, self.target_size, is_seg=True)
            
            # æ¨™æº–åŒ–å½±åƒ
            image = self.normalize_image(image)
            
            # æ¸…ç†æ¨™ç±¤
            mask = self.clean_labels(mask, mask_path.name)
            
            # æ·»åŠ é€šé“ç¶­åº¦
            image = image[np.newaxis, ...]
            
            # è½‰æ›ç‚º tensor
            image = torch.from_numpy(image).float()
            mask = torch.from_numpy(mask).long()
            
            # é©—è­‰
            mask_unique = torch.unique(mask)
            if not all(val in [0, 1] for val in mask_unique.tolist()):
                raise ValueError(f"Tensor åŒ…å«éäºŒå…ƒæ¨™ç±¤å€¼: {mask_unique.tolist()}")
            
            # æ‡‰ç”¨æ•¸æ“šå¢å¼·
            if self.augmentation_transform is not None:
                image, mask = self.augmentation_transform(image, mask)
            
            if self.external_transform is not None:
                image, mask = self.external_transform(image, mask)
            
            return {
                'image': image,
                'mask': mask,
                'image_path': str(img_path),
                'mask_path': str(mask_path)
            }
            
        except Exception as e:
            print(f"âŒ è™•ç†æª”æ¡ˆå° {img_path.name} / {mask_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise


# ==================== è³‡æ–™è¼‰å…¥å·¥å…·å‡½æ•¸ ====================
def create_data_loaders(data_root, batch_size=2, target_size=(64, 64, 64), 
                       num_workers=2, use_augmentation=True, augmentation_type='medical',
                       spacing=None, force_separate_z=None,
                       ):
    """
    å‰µå»ºè³‡æ–™è¼‰å…¥å™¨ï¼ˆä½¿ç”¨ nnUNet é¢¨æ ¼çš„ resamplingï¼‰
    
    Args:
        spacing: åŸå§‹è³‡æ–™çš„ spacing (z, y, x)ï¼Œä¾‹å¦‚ [3.0, 1.0, 1.0] è¡¨ç¤º z è»¸è§£æåº¦è¼ƒä½
        force_separate_z: å¼·åˆ¶æ˜¯å¦åˆ†é›¢ z è»¸è™•ç†ï¼ˆNone å‰‡è‡ªå‹•åˆ¤æ–·ï¼‰
    """
    data_loaders = {}
    
    for split in ['train', 'val', 'test']:
        split_path = Path(data_root) / split
        if split_path.exists():
            debug_mode = (split == 'train')
            
            dataset = MedicalImageDataset(
                data_root=data_root,
                split=split,
                target_size=target_size,
                num_classes=2,
                debug_labels=debug_mode,
                use_augmentation=use_augmentation,
                augmentation_type=augmentation_type,
                spacing=spacing,
                force_separate_z=force_separate_z,
            )
            
            shuffle = True if split == 'train' else False
            
            data_loaders[split] = DataLoader(
                dataset, 
                batch_size=batch_size, 
                shuffle=shuffle, 
                num_workers=num_workers,
                pin_memory=True
            )
        else:
            print(f"âš ï¸ {split} ç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³é")
    
    return data_loaders


# ==================== é™¤éŒ¯å·¥å…·å‡½æ•¸ ====================
def debug_data_loader(data_loader, num_samples=3):
    """é™¤éŒ¯è³‡æ–™è¼‰å…¥å™¨"""
    print("\n=== DataLoader é™¤éŒ¯è³‡è¨Š (nnUNet é¢¨æ ¼ resampling) ===")
    print(f"æ‰¹æ¬¡å¤§å°: {data_loader.batch_size}")
    print(f"è³‡æ–™é›†å¤§å°: {len(data_loader.dataset)}")
    print(f"æ‰¹æ¬¡æ•¸é‡: {len(data_loader)}")
    
    if hasattr(data_loader.dataset, 'augmentation_transform') and data_loader.dataset.augmentation_transform:
        print("ğŸ”„ æ•¸æ“šå¢å¼·: å·²å•Ÿç”¨")
    else:
        print("âŒ æ•¸æ“šå¢å¼·: æœªå•Ÿç”¨")
    
    for i, batch in enumerate(data_loader):
        if i >= num_samples:
            break
            
        images = batch['image']
        masks = batch['mask']
        
        print(f"\n--- æ‰¹æ¬¡ {i+1} ---")
        print(f"å½±åƒ shape: {images.shape}, dtype: {images.dtype}")
        print(f"æ¨™ç±¤ shape: {masks.shape}, dtype: {masks.dtype}")
        print(f"å½±åƒå€¼ç¯„åœ: [{images.min():.3f}, {images.max():.3f}]")
        print(f"æ¨™ç±¤å”¯ä¸€å€¼: {torch.unique(masks).tolist()}")
        
        unique_labels = torch.unique(masks).tolist()
        if set(unique_labels).issubset({0, 1}):
            print("âœ… æ¨™ç±¤æ­£ç¢ºï¼šåªåŒ…å«0å’Œ1")
            
            total_pixels = masks.numel()
            bg_pixels = (masks == 0).sum().item()
            fg_pixels = (masks == 1).sum().item()
            
            print(f"   èƒŒæ™¯(0): {bg_pixels} ({bg_pixels/total_pixels*100:.2f}%)")
            print(f"   å‰æ™¯(1): {fg_pixels} ({fg_pixels/total_pixels*100:.2f}%)")
        else:
            print(f"âŒ æ¨™ç±¤éŒ¯èª¤ï¼šåŒ…å«éäºŒå…ƒå€¼ {unique_labels}")
    
    print("=== é™¤éŒ¯å®Œæˆ ===\n")


# ==================== æ¸¬è©¦å‡½æ•¸ ====================
def test_nnunet_style_dataset():
    """æ¸¬è©¦ nnUNet é¢¨æ ¼çš„è³‡æ–™é›†"""
    data_root = r"D:\unet3d\dataset"
    
    print("ğŸ§ª æ¸¬è©¦ nnUNet é¢¨æ ¼ resampling...")
    
    # ç¯„ä¾‹ï¼šå‡è¨­ä½ çš„è³‡æ–™ z è»¸è§£æåº¦è¼ƒä½ï¼ˆå„å‘ç•°æ€§ï¼‰
    spacing = [3.0, 1.0, 1.0]  # (z, y, x)
    
    try:
        data_loaders = create_data_loaders(
            data_root=data_root,
            batch_size=1,
            target_size=(32, 64, 64),
            use_augmentation=True,
            augmentation_type='medical',
            spacing=spacing,  # æä¾› spacing è³‡è¨Š
            force_separate_z=None  # è‡ªå‹•åˆ¤æ–·
        )
        
        if 'train' in data_loaders:
            print("\nğŸ“Š æ¸¬è©¦çµæœ:")
            debug_data_loader(data_loaders['train'], num_samples=2)
        else:
            print("âš ï¸ æœªæ‰¾åˆ°è¨“ç·´é›†è³‡æ–™")
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


if __name__ == '__main__':
    test_nnunet_style_dataset()
