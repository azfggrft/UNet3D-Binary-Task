#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
3D UNet ç°¡åŒ–è¨“ç·´è…³æœ¬ - å« Warmup å’Œé€²éšå„ªåŒ–å™¨è¨­å®š
åªéœ€è¦èª¿æ•´åƒæ•¸ï¼Œæ‰€æœ‰è¨“ç·´å‡½æ•¸éƒ½åœ¨ trainer ä¸­
"""

import torch
import sys
import argparse
from pathlib import Path
import random
import numpy as np
import math

from src.network_architecture.net_module import *  # UNet3D æ¨¡å‹å®šç¾©
from trainer import *  # è¨“ç·´å·¥å…·å’Œ trainer
from src.network_architecture.unet3d import UNet3D
from src.data_processing_and_data_enhancement.dataload import MedicalImageDataset, create_data_loaders

"""
ğŸš€ Enhanced 3D UNet è¨“ç·´è…³æœ¬ - æ•´åˆè¦–è¦ºåŒ–åŠŸèƒ½èˆ‡ Warmup

æ–°å¢é€²éšåŠŸèƒ½ï¼š
âœ… Warmup å­¸ç¿’ç‡èª¿åº¦ - æ¼¸é€²å¼å­¸ç¿’ç‡æå‡
âœ… Momentum Warmup - å‹•é‡åƒæ•¸æ¼¸é€²èª¿æ•´
âœ… Bias åƒæ•¸ç‰¹æ®Šè™•ç† - ä¸åŒçš„å­¸ç¿’ç‡è¨­å®š
âœ… åƒæ•¸åˆ†çµ„å„ªåŒ– - æ¬Šé‡å’Œåç½®åˆ†åˆ¥è™•ç†
âœ… å¤šç¨®å­¸ç¿’ç‡èª¿åº¦å™¨ (Step, ReduceLROnPlateau, Cosine)
âœ… é€²éšå„ªåŒ–å™¨æ”¯æ´ (Adam, AdamW, SGD with momentum)

Warmup æ©Ÿåˆ¶èªªæ˜ï¼š
ğŸ”¥ å‰ N å€‹ epoch ä½¿ç”¨è¼ƒä½çš„å­¸ç¿’ç‡å’Œå‹•é‡ï¼Œé€æ¼¸æå‡åˆ°ç›®æ¨™å€¼
ğŸŒ¡ï¸  æœ‰åŠ©æ–¼ç©©å®šè¨“ç·´åˆæœŸï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸
âš–ï¸  Bias åƒæ•¸ä½¿ç”¨ä¸åŒçš„ warmup ç­–ç•¥

åƒæ•¸å»ºè­°ç¯„åœï¼š
ğŸ“ˆ momentum: 0.3-0.98 (SGD), 0.6-0.98 (Adam beta1)
ğŸ”¥ warmup_epochs: 1-5 epochs (å¯ä»¥æ˜¯å°æ•¸ï¼Œå¦‚ 2.5)
ğŸŒ¡ï¸  warmup_momentum: 0.0-0.95 (åˆå§‹å‹•é‡)
âš–ï¸  warmup_bias_lr: 0.0-0.2 (bias å­¸ç¿’ç‡ä¹˜æ•¸)

ä½¿ç”¨èªªæ˜:
1. åŸºæœ¬ä½¿ç”¨ï¼ˆå« warmupï¼‰ï¼š
   python train.py

2. è‡ªè¨‚ warmup åƒæ•¸ï¼š
   python train.py --warmup_epochs 3.0 --warmup_momentum 0.1 --momentum 0.9

3. åœç”¨ warmupï¼š
   python train.py --warmup_epochs 0

4. ä½¿ç”¨ä¸åŒèª¿åº¦å™¨ï¼š
   python train.py --scheduler cosine

5. å®Œæ•´é€²éšè¨­å®šï¼š
   python train.py --epochs 100 --lr 1e-3 --momentum 0.95 \
                   --warmup_epochs 2.5 --warmup_momentum 0.1 \
                   --warmup_bias_lr 0.15 --scheduler cosine

æ–°å¢è¼¸å‡ºå…§å®¹ï¼š
ğŸ“ˆ Warmup å­¸ç¿’ç‡å’Œå‹•é‡è®ŠåŒ–æ›²ç·š
ğŸ“Š ä¸åŒåƒæ•¸çµ„çš„å­¸ç¿’ç‡è¿½è¹¤
ğŸ”¥ Warmup éšæ®µçš„è©³ç´°æ—¥èªŒ

è¨“ç·´éšæ®µèªªæ˜ï¼š
1ï¸âƒ£ Warmup éšæ®µ (0 - warmup_epochs)ï¼š
   - å­¸ç¿’ç‡å¾ 0 ç·šæ€§å¢åŠ åˆ°ç›®æ¨™å€¼
   - Momentum å¾ warmup_momentum å¢åŠ åˆ°ç›®æ¨™å€¼
   - Bias åƒæ•¸ä½¿ç”¨ç‰¹æ®Šçš„å­¸ç¿’ç‡è¨­å®š

2ï¸âƒ£ æ­£å¸¸è¨“ç·´éšæ®µ (warmup_epochs - ç¸½epochs)ï¼š
   - ä½¿ç”¨ç›®æ¨™å­¸ç¿’ç‡å’Œå‹•é‡
   - æ ¹æ“šè¨­å®šçš„èª¿åº¦å™¨èª¿æ•´å­¸ç¿’ç‡

æ•ˆæœï¼š
âœ… æ›´ç©©å®šçš„è¨“ç·´é–‹å§‹
âœ… æ›´å¥½çš„æ”¶æ–‚æ€§èƒ½
âœ… æ¸›å°‘è¨“ç·´åˆæœŸçš„ä¸ç©©å®šæ€§
âœ… æ”¯æ´æ›´å¤§çš„å­¸ç¿’ç‡è¨­å®š

æ³¨æ„äº‹é …ï¼š
âš ï¸  Warmup æœƒç¨å¾®å»¶é•·è¨“ç·´æ™‚é–“
âš ï¸  éœ€è¦æ ¹æ“šè³‡æ–™é›†å¤§å°èª¿æ•´ warmup_epochs
âš ï¸  æŸäº›å°è³‡æ–™é›†å¯èƒ½ä¸éœ€è¦ warmup
âš ï¸  SGD å’Œ Adam çš„æœ€ä½³ momentum è¨­å®šä¸åŒ
"""


# ==================== Warmup å­¸ç¿’ç‡èª¿åº¦å™¨ ====================
class WarmupLRScheduler:
    """
    Warmup å­¸ç¿’ç‡èª¿åº¦å™¨
    æ”¯æ´ momentum å’Œ bias lr çš„ warmup
    """
    def __init__(self, optimizer, warmup_epochs, warmup_momentum, warmup_bias_lr, base_lr, base_momentum):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.warmup_momentum = warmup_momentum
        self.warmup_bias_lr = warmup_bias_lr
        self.base_lr = base_lr
        self.base_momentum = base_momentum
        self.current_epoch = 0
        
        # ä¿å­˜åŸå§‹åƒæ•¸çµ„è¨­å®š
        self.param_groups_info = []
        for i, param_group in enumerate(optimizer.param_groups):
            info = {
                'base_lr': param_group.get('lr', base_lr),
                'base_momentum': param_group.get('momentum', base_momentum),
                'is_bias': 'bias' in str(param_group.get('name', '')).lower()
            }
            self.param_groups_info.append(info)
    
    def step(self, epoch=None):
        """æ›´æ–°å­¸ç¿’ç‡å’Œ momentum"""
        if epoch is not None:
            self.current_epoch = epoch
        else:
            self.current_epoch += 1
        
        if self.current_epoch < self.warmup_epochs:
            # Warmup éšæ®µ - ä¿®å¾©ï¼šå¾ç¬¬1æ­¥é–‹å§‹ï¼Œè€Œä¸æ˜¯å¾0é–‹å§‹
            warmup_ratio = (self.current_epoch + 1) / self.warmup_epochs
            
            for i, param_group in enumerate(self.optimizer.param_groups):
                info = self.param_groups_info[i]
                
                # å­¸ç¿’ç‡ warmupï¼ˆbias å’Œä¸€èˆ¬åƒæ•¸ä¸åŒè™•ç†ï¼‰
                if info['is_bias']:
                    # bias åƒæ•¸ä½¿ç”¨ç‰¹æ®Šçš„ warmup
                    target_lr = self.warmup_bias_lr * info['base_lr']
                    param_group['lr'] = target_lr * warmup_ratio
                else:
                    # ä¸€èˆ¬åƒæ•¸çš„ç·šæ€§ warmup
                    param_group['lr'] = info['base_lr'] * warmup_ratio
                
                # Momentum warmupï¼ˆå¦‚æœå„ªåŒ–å™¨æ”¯æ´ï¼‰
                if 'momentum' in param_group:
                    param_group['momentum'] = self.warmup_momentum + (info['base_momentum'] - self.warmup_momentum) * warmup_ratio
                elif 'betas' in param_group:  # Adam ç³»åˆ—
                    # å°æ–¼ Adamï¼Œèª¿æ•´ beta1ï¼ˆç›¸ç•¶æ–¼ momentumï¼‰
                    original_betas = param_group['betas']
                    new_beta1 = self.warmup_momentum + (info['base_momentum'] - self.warmup_momentum) * warmup_ratio
                    param_group['betas'] = (new_beta1, original_betas[1])
        
        else:
            # Warmup å®Œæˆï¼Œæ¢å¾©æ­£å¸¸è¨­å®š
            for i, param_group in enumerate(self.optimizer.param_groups):
                info = self.param_groups_info[i]
                param_group['lr'] = info['base_lr']
                
                if 'momentum' in param_group:
                    param_group['momentum'] = info['base_momentum']
                elif 'betas' in param_group:
                    param_group['betas'] = (info['base_momentum'], param_group['betas'][1])
    
    def get_lr(self):
        """å–å¾—ç•¶å‰å­¸ç¿’ç‡"""
        return [param_group['lr'] for param_group in self.optimizer.param_groups]
    
    def get_momentum(self):
        """å–å¾—ç•¶å‰ momentum"""
        momenta = []
        for param_group in self.optimizer.param_groups:
            if 'momentum' in param_group:
                momenta.append(param_group['momentum'])
            elif 'betas' in param_group:
                momenta.append(param_group['betas'][0])
            else:
                momenta.append(0.0)
        return momenta
    
    def get_lr(self):
        """å–å¾—ç•¶å‰å­¸ç¿’ç‡"""
        return [param_group['lr'] for param_group in self.optimizer.param_groups]
    
    def get_momentum(self):
        """å–å¾—ç•¶å‰ momentum"""
        momenta = []
        for param_group in self.optimizer.param_groups:
            if 'momentum' in param_group:
                momenta.append(param_group['momentum'])
            elif 'betas' in param_group:
                momenta.append(param_group['betas'][0])
            else:
                momenta.append(0.0)
        return momenta


# ==================== è¨“ç·´åƒæ•¸é…ç½® ====================
def get_config():
    """
    ğŸ”§ åœ¨é€™è£¡èª¿æ•´æ‰€æœ‰è¨“ç·´åƒæ•¸
    """
    config = {
        # === è³‡æ–™ç›¸é—œåƒæ•¸ ===
        'data_root': r"D:\unet3d_chinese\dataset",  # ğŸ”§ ä¿®æ”¹ç‚ºä½ çš„è³‡æ–™æ ¹ç›®éŒ„
        'target_size': (64, 64, 64),     # ğŸ”§ ç›®æ¨™å½±åƒå°ºå¯¸ (D, H, W)
        'batch_size': 8,                 # ğŸ”§ æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ“šé¡¯å¡è¨˜æ†¶é«”èª¿æ•´ï¼‰
        'num_workers': 4,                # ğŸ”§ è³‡æ–™è¼‰å…¥åŸ·è¡Œç·’æ•¸
        'use_augmentation': False,        # ğŸ”§ æ˜¯å¦å•Ÿç”¨æ•¸æ“šå¢å¼·ï¼ˆåªå°è¨“ç·´é›†ï¼‰
        'augmentation_type': 'custom',    # ğŸ”§ æ•¸æ“šå¢å¼·é¡å‹ ('light', 'medium', 'heavy', 'medical', 'medical_heavy', 'custom')
        
        # === æ¨¡å‹ç›¸é—œåƒæ•¸ ===
        'n_channels': 1,                 # ğŸ”§ è¼¸å…¥é€šé“æ•¸ï¼ˆç°éšå½±åƒç‚º1ï¼ŒRGBç‚º3ï¼‰
        'n_classes': 2,                  # ğŸ”§ è¼¸å‡ºé¡åˆ¥æ•¸ï¼ˆäºŒåˆ†é¡ç‚º2ï¼Œå¤šåˆ†é¡æ ¹æ“šéœ€æ±‚ï¼‰
        'base_channels': 64,             # ğŸ”§ åŸºç¤é€šé“æ•¸ï¼ˆå¯ä»¥æ˜¯16, 32, 64ï¼‰
        'num_groups': 8,                 # ğŸ”§ GroupNorm çµ„æ•¸
        'bilinear': False,               # ğŸ”§ æ˜¯å¦ä½¿ç”¨é›™ç·šæ€§ä¸Šæ¡æ¨£
        
        # === è¨“ç·´ç›¸é—œåƒæ•¸ ===
        'num_epochs': 300,               # ğŸ”§ è¨“ç·´ epoch æ•¸
        'learning_rate': 1e-3,           # ğŸ”§ å­¸ç¿’ç‡ (SGD=1E-2, Adam=1E-3)
        'weight_decay': 5e-4,            # ğŸ”§ æ¬Šé‡è¡°æ¸›
        'optimizer': 'adam',             # ğŸ”§ å„ªåŒ–å™¨ ('adam', 'adamw', 'sgd')
        
        # === é€²éšå„ªåŒ–å™¨åƒæ•¸ ===
        'momentum': 0.937,               # ğŸ”§ SGD momentum æˆ– Adam beta1 (0.3, 0.6, 0.98)
        'warmup_epochs': 3.0,            # ğŸ”§ warmup epochsï¼ˆæ”¯æ´å°æ•¸ï¼‰
        'warmup_momentum': 0.8,          # ğŸ”§ warmup åˆå§‹ momentum (0.0, 0.95)
        'warmup_bias_lr': 0.1,           # ğŸ”§ warmup åˆå§‹ bias lr ä¹˜æ•¸ (0.0, 0.2)
        
        # === è¦–è¦ºåŒ–ç›¸é—œåƒæ•¸ ===
        'enable_visualization': True,    # ğŸ”§ æ˜¯å¦å•Ÿç”¨è¦–è¦ºåŒ–
        'plot_interval': 20,             # ğŸ”§ è¦–è¦ºåŒ–æ›´æ–°é–“éš”ï¼ˆæ¯å¹¾å€‹epochï¼‰
        'early_stopping_patience': 31,   # ğŸ”§ æ—©åœè€å¿ƒå€¼
        
        # === æå¤±å‡½æ•¸åƒæ•¸ ===
        'loss_type': 'combined',         # ğŸ”§ æå¤±å‡½æ•¸é¡å‹ ('dice', 'ce', 'combined')
        'ce_weight': 0.4,                # ğŸ”§ Cross Entropy æ¬Šé‡
        'dice_weight': 0.6,              # ğŸ”§ Dice Loss æ¬Šé‡
        
        # === å­¸ç¿’ç‡èª¿åº¦å™¨ ===
        'scheduler': 'reduce_on_plateau', # ğŸ”§ èª¿åº¦å™¨é¡å‹ ('step', 'reduce_on_plateau', 'cosine', None)
        'scheduler_patience': 10,         # ğŸ”§ ReduceLROnPlateau è€å¿ƒå€¼
        'scheduler_factor': 0.5,          # ğŸ”§ å­¸ç¿’ç‡è¡°æ¸›å› å­
        'step_size': 10,                  # ğŸ”§ StepLR æ­¥é•·
        'cosine_t_max': None,             # ğŸ”§ Cosine èª¿åº¦å™¨æœ€å¤§é€±æœŸï¼ˆNoneå‰‡ä½¿ç”¨ç¸½epochæ•¸ï¼‰
        
        # === ä¿å­˜å’Œæ—¥èªŒ ===
        'save_dir': r"D:\unet3d_chinese\train_end",     # ğŸ”§ æ¨¡å‹ä¿å­˜ç›®éŒ„
        'log_interval': 1,               # ğŸ”§ æ—¥èªŒè¼¸å‡ºé–“éš”
        'save_interval': 200,            # ğŸ”§ æ¨¡å‹ä¿å­˜é–“éš”
        'resume_from': None,             # ğŸ”§ å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´ï¼ˆè·¯å¾‘æˆ–Noneï¼‰
        
        # === å…¶ä»–è¨­å®š ===
        'seed': 42,                      # ğŸ”§ éš¨æ©Ÿç¨®å­
        'device': 'auto',                # ğŸ”§ è¨­å‚™é¸æ“‡ ('auto', 'cpu', 'cuda:0')
        'run_test': True,                # ğŸ”§ è¨“ç·´å®Œæˆå¾Œæ˜¯å¦åŸ·è¡Œæ¸¬è©¦
    }
    return config

def set_seed(seed):
    """è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def validate_config(config):
    """é©—è­‰é…ç½®åƒæ•¸"""
    print("ğŸ” é©—è­‰é…ç½®åƒæ•¸...")
    
    data_root = Path(config['data_root'])
    if not data_root.exists():
        raise FileNotFoundError(f"âŒ è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨: {data_root}")
    
    train_images = data_root / 'train' / 'images'
    train_labels = data_root / 'train' / 'labels'
    
    if not train_images.exists():
        raise FileNotFoundError(f"âŒ è¨“ç·´å½±åƒç›®éŒ„ä¸å­˜åœ¨: {train_images}")
    if not train_labels.exists():
        raise FileNotFoundError(f"âŒ è¨“ç·´æ¨™ç±¤ç›®éŒ„ä¸å­˜åœ¨: {train_labels}")
    
    print("âœ… åŸºæœ¬é…ç½®é©—è­‰é€šé")
    
    for split in ['val', 'test']:
        images_dir = data_root / split / 'images'
        labels_dir = data_root / split / 'labels'
        if images_dir.exists() and labels_dir.exists():
            print(f"âœ… æ‰¾åˆ° {split} è³‡æ–™é›†")
        else:
            print(f"âš ï¸  {split} è³‡æ–™é›†ä¸å®Œæ•´æˆ–ä¸å­˜åœ¨")
    
    # é©—è­‰æ–°å¢çš„åƒæ•¸
    if not (0.0 <= config['momentum'] <= 1.0):
        print(f"âš ï¸  è­¦å‘Š: momentum å€¼ {config['momentum']} è¶…å‡ºå»ºè­°ç¯„åœ [0.0, 1.0]")
    
    if config['warmup_epochs'] < 0:
        print(f"âŒ éŒ¯èª¤: warmup_epochs ä¸èƒ½ç‚ºè² æ•¸: {config['warmup_epochs']}")
        raise ValueError("warmup_epochs å¿…é ˆ >= 0")
    
    if not (0.0 <= config['warmup_momentum'] <= 1.0):
        print(f"âš ï¸  è­¦å‘Š: warmup_momentum å€¼ {config['warmup_momentum']} è¶…å‡ºå»ºè­°ç¯„åœ [0.0, 1.0]")
    
    if not (0.0 <= config['warmup_bias_lr'] <= 1.0):
        print(f"âš ï¸  è­¦å‘Š: warmup_bias_lr å€¼ {config['warmup_bias_lr']} è¶…å‡ºå»ºè­°ç¯„åœ [0.0, 1.0]")
    
    print("ğŸ” åŸ·è¡Œè³‡æ–™å®Œæ•´æ€§æª¢æŸ¥...")
    check_data_integrity(config)

def check_data_integrity(config):
    """æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§å’Œæ¨™ç±¤å€¼ç¯„åœ"""
    try:
        test_dataset = MedicalImageDataset(
            data_root=config['data_root'],
            split='train',
            target_size=config['target_size'],
            use_augmentation=config['use_augmentation'],
            augmentation_type=config['augmentation_type']
        )
        
        if len(test_dataset) > 0:
            sample = test_dataset[0]
            image = sample['image']
            mask = sample['mask']
            
            print(f"ğŸ“Š å½±åƒå½¢ç‹€: {image.shape}")
            print(f"ğŸ“Š æ¨™ç±¤å½¢ç‹€: {mask.shape}")
            print(f"ğŸ“Š å½±åƒå€¼ç¯„åœ: [{image.min().item():.3f}, {image.max().item():.3f}]")
            print(f"ğŸ“Š æ¨™ç±¤å€¼ç¯„åœ: [{mask.min().item()}, {mask.max().item()}]")
            print(f"ğŸ“Š å”¯ä¸€æ¨™ç±¤å€¼: {torch.unique(mask).tolist()}")
            
            max_label = mask.max().item()
            if max_label >= config['n_classes']:
                print(f"âš ï¸  è­¦å‘Š: æœ€å¤§æ¨™ç±¤å€¼ {max_label} è¶…å‡ºé¡åˆ¥æ•¸ {config['n_classes']}")
                print(f"ğŸ’¡ å»ºè­°: å°‡ n_classes è¨­ç‚º {max_label + 1} æˆ–æª¢æŸ¥æ¨™ç±¤è³‡æ–™")
            
            min_label = mask.min().item()
            if min_label < 0:
                print(f"âŒ éŒ¯èª¤: ç™¼ç¾è² æ•¸æ¨™ç±¤å€¼ {min_label}")
                raise ValueError("æ¨™ç±¤å€¼ä¸èƒ½ç‚ºè² æ•¸")
            
            print("âœ… è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥é€šé")
        else:
            print("âŒ æ‰¾ä¸åˆ°ä»»ä½•è¨“ç·´è³‡æ–™")
            
    except Exception as e:
        print(f"âš ï¸  è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥å¤±æ•—: {e}")
        print("ğŸ’¡ å»ºè­°ç¹¼çºŒåŸ·è¡Œï¼Œä½†è«‹æ³¨æ„å¯èƒ½å‡ºç¾çš„å•é¡Œ")

def print_config_summary(config):
    """é¡¯ç¤ºé…ç½®æ‘˜è¦"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ è¨“ç·´é…ç½®æ‘˜è¦")
    print("=" * 60)
    print(f"ğŸ“ è³‡æ–™æ ¹ç›®éŒ„: {config['data_root']}")
    print(f"ğŸ¯ å½±åƒå°ºå¯¸: {config['target_size']}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"ğŸ—ï¸  æ¨¡å‹é€šé“: {config['n_channels']} -> {config['n_classes']}")
    print(f"ğŸ® è¨“ç·´è¼ªæ•¸: {config['num_epochs']}")
    print(f"âš¡ å­¸ç¿’ç‡: {config['learning_rate']}")
    print(f"ğŸš€ å„ªåŒ–å™¨: {config['optimizer'].upper()}")
    print(f"ğŸ“ˆ Momentum: {config['momentum']}")
    print(f"ğŸ”¥ Warmup epochs: {config['warmup_epochs']}")
    print(f"ğŸŒ¡ï¸  Warmup momentum: {config['warmup_momentum']}")
    print(f"âš–ï¸  Warmup bias lr: {config['warmup_bias_lr']}")
    print(f"ğŸ¯ æå¤±å‡½æ•¸: {config['loss_type']}")
    print(f"ğŸ“… èª¿åº¦å™¨: {config['scheduler']}")
    print(f"ğŸ’¾ ä¿å­˜ç›®éŒ„: {config['save_dir']}")
    print(f"ğŸ¨ è¦–è¦ºåŒ–: {'å•Ÿç”¨' if config['enable_visualization'] else 'åœç”¨'}")
    if config['enable_visualization']:
        print(f"ğŸ“Š è¦–è¦ºåŒ–é–“éš”: æ¯ {config['plot_interval']} epochs")
    print(f"ğŸ”„ æ•¸æ“šå¢å¼·: {'å•Ÿç”¨' if config['use_augmentation'] else 'åœç”¨'}")
    if config['use_augmentation']:
        print(f"ğŸ­ å¢å¼·é¡å‹: {config['augmentation_type']}")
    print("=" * 60)

def setup_optimizer_with_param_groups(model, config):
    """
    è¨­ç½®å„ªåŒ–å™¨ï¼Œæ”¯æ´ä¸åŒåƒæ•¸çµ„çš„ä¸åŒè¨­å®š
    ç‚º bias åƒæ•¸å’Œå…¶ä»–åƒæ•¸è¨­å®šä¸åŒçš„å­¸ç¿’ç‡
    """
    # åˆ†é›¢ bias åƒæ•¸å’Œå…¶ä»–åƒæ•¸
    bias_params = []
    other_params = []
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            if 'bias' in name.lower():
                bias_params.append(param)
            else:
                other_params.append(param)
    
    # å»ºç«‹åƒæ•¸çµ„
    param_groups = [
        {
            'params': other_params,
            'lr': config['learning_rate'],
            'weight_decay': config['weight_decay'],
            'name': 'weights'
        },
        {
            'params': bias_params,
            'lr': config['learning_rate'],
            'weight_decay': 0.0,  # bias é€šå¸¸ä¸ä½¿ç”¨ weight decay
            'name': 'bias'
        }
    ]
    
    print(f"ğŸ“Š åƒæ•¸åˆ†çµ„: æ¬Šé‡åƒæ•¸ {len(other_params)} å€‹, bias åƒæ•¸ {len(bias_params)} å€‹")
    
    # æ ¹æ“šå„ªåŒ–å™¨é¡å‹è¨­å®š
    if config['optimizer'].lower() == 'adam':
        optimizer = torch.optim.Adam(
            param_groups,
            betas=(config['momentum'], 0.999)  # ä½¿ç”¨ momentum ä½œç‚º beta1
        )
    elif config['optimizer'].lower() == 'adamw':
        optimizer = torch.optim.AdamW(
            param_groups,
            betas=(config['momentum'], 0.999)  # ä½¿ç”¨ momentum ä½œç‚º beta1
        )
    elif config['optimizer'].lower() == 'sgd':
        # ç‚º SGD æ·»åŠ  momentum åƒæ•¸
        for group in param_groups:
            group['momentum'] = config['momentum']
        
        optimizer = torch.optim.SGD(param_groups)
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„å„ªåŒ–å™¨: {config['optimizer']}")
    
    return optimizer

def create_lr_scheduler(optimizer, config, warmup_scheduler=None):
    """å‰µå»ºå­¸ç¿’ç‡èª¿åº¦å™¨ï¼ˆåœ¨ warmup ä¹‹å¾Œä½¿ç”¨ï¼‰"""
    if not config['scheduler']:
        return None
    
    if config['scheduler'] == 'step':
        from torch.optim.lr_scheduler import StepLR
        scheduler = StepLR(
            optimizer,
            step_size=config['step_size'],
            gamma=config['scheduler_factor']
        )
        print(f"ğŸ“… ä¸»èª¿åº¦å™¨: StepLR (æ¯ {config['step_size']} epoch è¡°æ¸› {config['scheduler_factor']})")
        
    elif config['scheduler'] == 'reduce_on_plateau':
        from torch.optim.lr_scheduler import ReduceLROnPlateau
        scheduler = ReduceLROnPlateau(
            optimizer,
            mode='max',
            patience=config['scheduler_patience'],
            factor=config['scheduler_factor'],
        )
        print(f"ğŸ“‰ ä¸»èª¿åº¦å™¨: ReduceLROnPlateau (è€å¿ƒå€¼: {config['scheduler_patience']})")
        
    elif config['scheduler'] == 'cosine':
        from torch.optim.lr_scheduler import CosineAnnealingLR
        t_max = config.get('cosine_t_max', config['num_epochs'])
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=t_max,
            eta_min=config['learning_rate'] * 0.01  # æœ€å°å­¸ç¿’ç‡ç‚ºåˆå§‹å€¼çš„1%
        )
        print(f"ğŸŒŠ ä¸»èª¿åº¦å™¨: CosineAnnealingLR (T_max: {t_max})")
        
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„èª¿åº¦å™¨: {config['scheduler']}")
    
    return scheduler

def create_trainer_from_config(config):
    """æ ¹æ“šé…ç½®å‰µå»ºè¨“ç·´å™¨"""
    print("ğŸ—ï¸  æ ¹æ“šé…ç½®å‰µå»ºå¢å¼·ç‰ˆè¨“ç·´å™¨...")
    
    # è¨­ç½®è¨­å‚™
    if config['device'] == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(config['device'])
    
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è¨­å‚™: {device}")
    if device.type == 'cuda':
        print(f"ğŸ”¢ GPU åç¨±: {torch.cuda.get_device_name(device)}")
        print(f"ğŸ’¾ GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(device).total_memory / 1024**3:.1f} GB")
    
    # å‰µå»ºæ¨¡å‹
    print("ğŸ—ï¸  å»ºç«‹æ¨¡å‹...")
    model = UNet3D(
        n_channels=config['n_channels'],
        n_classes=config['n_classes'],
        base_channels=config['base_channels'],
        num_groups=config['num_groups'],
        bilinear=config['bilinear']
    ).to(device)
    
    total_params, trainable_params = model.get_model_size()
    total_params, trainable_params = model.get_model_size()
    print(f"ğŸ“Š æ¨¡å‹åƒæ•¸: {total_params:,} ({trainable_params:,} å¯è¨“ç·´)")
    print(f"ğŸ’¾ ä¼°è¨ˆå¤§å°: {total_params * 4 / 1024 / 1024:.1f} MB (FP32)")

    # è¨ˆç®—æ¨¡å‹è¨ˆç®—é‡
    try:
        from thop import profile
        sample_input = torch.randn(1, config['n_channels'], *config['target_size']).to(device)
        flops, _ = profile(model, inputs=(sample_input,), verbose=False)
        # é™¤ä»¥2ä¾†ä¿®æ­£é‡è¤‡è¨ˆç®—ï¼ˆMAC vs FLOPsï¼‰
        flops = flops / 2 / 1e9  # è½‰æ›ç‚º GFLOPs
        print(f"ğŸ”¢ æ¨¡å‹è¨ˆç®—é‡: {flops:.3f}G")
        del sample_input
        if device.type == 'cuda':
            torch.cuda.empty_cache()
    except:
        print("âš ï¸ ç„¡æ³•è¨ˆç®— FLOPs (éœ€å®‰è£ thop: pip install thop)")
     
    # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„æ•¸æ“šå¢å¼·åƒæ•¸ï¼‰
    print("ğŸ“ å»ºç«‹è³‡æ–™è¼‰å…¥å™¨...")
    data_loaders = create_data_loaders(
        data_root=config['data_root'],
        batch_size=config['batch_size'],
        target_size=config['target_size'],
        num_workers=config['num_workers'],
        use_augmentation=config['use_augmentation'],
        augmentation_type=config['augmentation_type']
    )
    
    # è¨­ç½®å„ªåŒ–å™¨ï¼ˆæ”¯æ´åƒæ•¸åˆ†çµ„ï¼‰
    print("âš¡ è¨­ç½®é€²éšå„ªåŒ–å™¨...")
    optimizer = setup_optimizer_with_param_groups(model, config)
    print(f"âš¡ å„ªåŒ–å™¨: {config['optimizer'].upper()}")
    print(f"ğŸ“ˆ åŸºç¤å­¸ç¿’ç‡: {config['learning_rate']}")
    print(f"ğŸ“ˆ Momentum/Beta1: {config['momentum']}")
    
    # è¨­ç½® Warmup èª¿åº¦å™¨
    warmup_scheduler = None
    if config['warmup_epochs'] > 0:
        warmup_scheduler = WarmupLRScheduler(
            optimizer=optimizer,
            warmup_epochs=config['warmup_epochs'],
            warmup_momentum=config['warmup_momentum'],
            warmup_bias_lr=config['warmup_bias_lr'],
            base_lr=config['learning_rate'],
            base_momentum=config['momentum']
        )
        print(f"ğŸ”¥ Warmup èª¿åº¦å™¨: {config['warmup_epochs']} epochs")
        print(f"ğŸŒ¡ï¸  Warmup momentum: {config['warmup_momentum']} -> {config['momentum']}")
        print(f"âš–ï¸  Warmup bias lr ä¹˜æ•¸: {config['warmup_bias_lr']}")
    
    # è¨­ç½®ä¸»å­¸ç¿’ç‡èª¿åº¦å™¨
    main_scheduler = create_lr_scheduler(optimizer, config, warmup_scheduler)
    
    # è¨­ç½®æå¤±å‡½æ•¸
    if config['loss_type'] == 'dice':
        criterion = DiceLoss()
        print("ğŸ¯ æå¤±å‡½æ•¸: Dice Loss")
        
    elif config['loss_type'] == 'ce':
        criterion = torch.nn.CrossEntropyLoss()
        print("ğŸ¯ æå¤±å‡½æ•¸: Cross Entropy Loss")
        
    elif config['loss_type'] == 'combined':
        criterion = CombinedLoss(
            ce_weight=config['ce_weight'],
            dice_weight=config['dice_weight']
        )
        print(f"ğŸ¯ æå¤±å‡½æ•¸: Combined Loss (CE: {config['ce_weight']}, Dice: {config['dice_weight']})")
        
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æå¤±å‡½æ•¸: {config['loss_type']}")
    
    # å‰µå»ºå¢å¼·ç‰ˆè¨“ç·´å™¨ä¸¦å‚³éå®Œæ•´é…ç½®
    trainer = EnhancedUNet3DTrainer(
        model=model,
        data_loaders=data_loaders,
        optimizer=optimizer,
        criterion=criterion,
        device=device,
        save_dir=config['save_dir'],
        log_interval=config['log_interval'],
        save_interval=config['save_interval'],
        scheduler=main_scheduler,
        visualize=config['enable_visualization'],
        plot_interval=config['plot_interval'],
        training_config=config  # ğŸ”‘ é—œéµï¼šå‚³éå®Œæ•´é…ç½®
    )
    
    # å°‡ warmup scheduler æ·»åŠ åˆ° trainer ä¸­
    if warmup_scheduler:
        trainer.warmup_scheduler = warmup_scheduler
        trainer.warmup_epochs = config['warmup_epochs']
        print("âœ… Warmup èª¿åº¦å™¨å·²æ•´åˆåˆ°è¨“ç·´å™¨ä¸­")
    
    return trainer

def main():
    """ä¸»è¨“ç·´å‡½æ•¸"""
    print("ğŸš€ Enhanced 3D UNet è¨“ç·´ç³»çµ±å•Ÿå‹• (å« Warmup)")
    print("=" * 60)
    
    # è¨­ç½® CUDA èª¿è©¦æ¨¡å¼
    import os
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    print("ğŸ”§ å•Ÿç”¨ CUDA èª¿è©¦æ¨¡å¼")
    
    config = get_config()
    
    set_seed(config['seed'])
    print(f"ğŸ² è¨­ç½®éš¨æ©Ÿç¨®å­: {config['seed']}")
    
    validate_config(config)
    print_config_summary(config)
    
    trainer = create_trainer_from_config(config)
    
    print("\nğŸš€ é–‹å§‹å¢å¼·ç‰ˆè¨“ç·´ (å« Warmup)...")
    print("=" * 70)
    
    try:
        # åŸ·è¡Œè¨“ç·´ï¼ˆåŒ…å«è¦–è¦ºåŒ–å’Œæ—©åœï¼‰
        trainer.train(
            num_epochs=config['num_epochs'],
            resume_from=config['resume_from'],
            early_stopping_patience=config['early_stopping_patience']
        )
        
        # è¨“ç·´å®Œæˆå¾Œé€²è¡Œæ¸¬è©¦
        if config['run_test'] and 'test' in trainer.data_loaders:
            print("\n" + "=" * 70)
            print("é–‹å§‹æ¸¬è©¦æœ€ä½³æ¨¡å‹...")
            best_model_path = Path(config['save_dir']) / 'best_model.pth'
            if best_model_path.exists():
                # åŸ·è¡Œæ¸¬è©¦ä¸¦è‡ªå‹•ä¿å­˜çµæœ
                test_results = trainer.test(str(best_model_path), save_results=True)
                
                if test_results:
                    # åœ¨æ§åˆ¶å°é¡¯ç¤ºçµæœ
                    print(f"\næœ€çµ‚æ¸¬è©¦çµæœ:")
                    print(f"å¹³å‡æå¤±: {test_results['avg_loss']:.4f}")
                    print(f"å¹³å‡ Dice åˆ†æ•¸: {test_results['avg_dice']:.4f}")
                    print(f"Dice åˆ†æ•¸ç™¾åˆ†æ¯”: {test_results['avg_dice'] * 100:.2f}%")
                
            else:
                print("æ‰¾ä¸åˆ°æœ€ä½³æ¨¡å‹æª”æ¡ˆ")
        
        print("\n" + "ğŸ‰" * 20)
        print("âœ… æ‰€æœ‰ä»»å‹™å®Œæˆï¼")
        print("ğŸ“Š è¨“ç·´æ›²ç·šå’Œè¦–è¦ºåŒ–çµæœå·²ä¿å­˜è‡³: " + str(Path(config['save_dir']) / 'visualizations'))
        print("ğŸ‰" * 20)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
        print("ğŸ’¾ ä¿å­˜ä¸­æ–·ç‹€æ…‹...")
        try:
            trainer.save_checkpoint(len(trainer.history['train_loss']), is_best=False)
            print("âœ… ç‹€æ…‹å·²ä¿å­˜ï¼Œå¯ä½¿ç”¨ resume_from åƒæ•¸æ¢å¾©è¨“ç·´")
        except:
            print("âŒ ä¿å­˜ç‹€æ…‹å¤±æ•—")
        
    except Exception as e:
        print(f"\nâŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ å»ºè­°æª¢æŸ¥ï¼š")
        print("   - æ¨™ç±¤å€¼æ˜¯å¦åœ¨æ­£ç¢ºç¯„åœå…§ [0, n_classes-1]")
        print("   - è³‡æ–™æ ¼å¼æ˜¯å¦æ­£ç¢º")
        print("   - é¡¯å¡è¨˜æ†¶é«”æ˜¯å¦è¶³å¤ ï¼ˆå¯é™ä½ batch_sizeï¼‰")
        print("   - æª”æ¡ˆè·¯å¾‘æ˜¯å¦æ­£ç¢º")
        print("   - æ˜¯å¦æœ‰ NaN æˆ–ç•°å¸¸å€¼åœ¨è³‡æ–™ä¸­")

def parse_args():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(
        description='Enhanced 3D UNet è¨“ç·´è…³æœ¬ - æ•´åˆè¦–è¦ºåŒ–åŠŸèƒ½èˆ‡ Warmup',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument('--data_root', type=str, help='è³‡æ–™æ ¹ç›®éŒ„')
    parser.add_argument('--epochs', type=int, help='è¨“ç·´ epoch æ•¸')
    parser.add_argument('--batch_size', type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, help='å­¸ç¿’ç‡')
    parser.add_argument('--resume', type=str, help='å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´')
    parser.add_argument('--save_dir', type=str, help='æ¨¡å‹ä¿å­˜ç›®éŒ„')
    parser.add_argument('--device', type=str, help='è¨ˆç®—è¨­å‚™')
    parser.add_argument('--n_classes', type=int, help='è¼¸å‡ºé¡åˆ¥æ•¸')
    parser.add_argument('--loss_type', type=str, choices=['dice', 'ce', 'combined'], help='æå¤±å‡½æ•¸é¡å‹')
    parser.add_argument('--no_viz', action='store_true', help='åœç”¨è¦–è¦ºåŒ–åŠŸèƒ½')
    parser.add_argument('--plot_interval', type=int, help='è¦–è¦ºåŒ–æ›´æ–°é–“éš”')
    parser.add_argument('--no_augmentation', action='store_true', help='åœç”¨æ•¸æ“šå¢å¼·')
    parser.add_argument('--augmentation_type', type=str, 
                        choices=['light', 'medium', 'heavy', 'medical', 'medical_heavy'],
                        help='æ•¸æ“šå¢å¼·é¡å‹')
    
    # æ–°å¢ Warmup ç›¸é—œåƒæ•¸
    parser.add_argument('--momentum', type=float, help='SGD momentum æˆ– Adam beta1')
    parser.add_argument('--warmup_epochs', type=float, help='Warmup epochs æ•¸ï¼ˆæ”¯æ´å°æ•¸ï¼‰')
    parser.add_argument('--warmup_momentum', type=float, help='Warmup åˆå§‹ momentum')
    parser.add_argument('--warmup_bias_lr', type=float, help='Warmup bias å­¸ç¿’ç‡ä¹˜æ•¸')
    parser.add_argument('--scheduler', type=str, 
                        choices=['step', 'reduce_on_plateau', 'cosine', None],
                        help='å­¸ç¿’ç‡èª¿åº¦å™¨é¡å‹')
    
    return parser.parse_args()

def update_config_from_args(config, args):
    """ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸æ›´æ–°é…ç½®"""
    updated = False
    
    if args.data_root:
        config['data_root'] = args.data_root
        updated = True
    if args.epochs:
        config['num_epochs'] = args.epochs
        updated = True
    if args.batch_size:
        config['batch_size'] = args.batch_size
        updated = True
    if args.lr:
        config['learning_rate'] = args.lr
        updated = True
    if args.resume:
        config['resume_from'] = args.resume
        updated = True
    if args.save_dir:
        config['save_dir'] = args.save_dir
        updated = True
    if args.device:
        config['device'] = args.device
        updated = True
    if args.n_classes:
        config['n_classes'] = args.n_classes
        updated = True
    if args.loss_type:
        config['loss_type'] = args.loss_type
        updated = True
    if args.no_viz:
        config['enable_visualization'] = False
        updated = True
    if args.plot_interval:
        config['plot_interval'] = args.plot_interval
        updated = True
    if args.no_augmentation:
        config['use_augmentation'] = False
        updated = True
    if args.augmentation_type:
        config['augmentation_type'] = args.augmentation_type
        updated = True
    
    # æ–°å¢ Warmup åƒæ•¸æ›´æ–°
    if args.momentum is not None:
        config['momentum'] = args.momentum
        updated = True
    if args.warmup_epochs is not None:
        config['warmup_epochs'] = args.warmup_epochs
        updated = True
    if args.warmup_momentum is not None:
        config['warmup_momentum'] = args.warmup_momentum
        updated = True
    if args.warmup_bias_lr is not None:
        config['warmup_bias_lr'] = args.warmup_bias_lr
        updated = True
    if args.scheduler is not None:
        config['scheduler'] = args.scheduler
        updated = True
    
    if updated:
        print("ğŸ“ ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸è¦†è“‹é…ç½®")
    
    return config

if __name__ == '__main__':
    args = parse_args()
    config = get_config()
    config = update_config_from_args(config, args)
    
    globals()['get_config'] = lambda: config
    main()

