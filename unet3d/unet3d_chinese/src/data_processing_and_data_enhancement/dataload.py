import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import nibabel as nib
from torch.utils.data import Dataset, DataLoader
import os
from pathlib import Path
import sys
# å°å…¥ä½ çš„æ¨¡çµ„ï¼ˆæ ¹æ“šå¯¦éš›è·¯å¾‘èª¿æ•´ï¼‰
sys.path.append(r"D:\unet3d")
from .augmentation_function import *
import warnings
warnings.filterwarnings('ignore')

#i love you~~~~~~~~~~~~~~
# ==================== æ”¹è‰¯ç‰ˆ NII.GZ æª”æ¡ˆè®€å– Dataset ====================
class MedicalImageDataset(Dataset):
    """
    é†«å­¸å½±åƒ NII.GZ æª”æ¡ˆè®€å– Dataset (æ•´åˆæ•¸æ“šå¢å¼·åŠŸèƒ½ï¼Œæ”¯æ´3Då’Œ4Dåœ–åƒ)
    æ”¯æ´æ¨™æº–ç›®éŒ„çµæ§‹ï¼š
    data_root/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ images/
    â”‚   â””â”€â”€ labels/
    â”œâ”€â”€ val/
    â”‚   â”œâ”€â”€ images/
    â”‚   â””â”€â”€ labels/
    â””â”€â”€ test/
        â”œâ”€â”€ images/
        â””â”€â”€ labels/
    """
    def __init__(self, data_root, split='train', image_suffix=['.nii.gz', '.nii'], 
             mask_suffix=['.nii.gz', '.nii'], transform=None, target_size=None, 
             num_classes=None, debug_labels=True, use_augmentation=True,
             augmentation_type='medical'):
        """
        Args:
            data_root: è³‡æ–™æ ¹ç›®éŒ„
            split: è³‡æ–™åˆ†å‰² ('train', 'val', 'test')
            image_suffix: å½±åƒæª”æ¡ˆå¾Œç¶´ï¼Œå¯ä»¥æ˜¯å­—ä¸²æˆ–åˆ—è¡¨ (é è¨­: ['.nii.gz', '.nii'])
            mask_suffix: æ¨™ç±¤æª”æ¡ˆå¾Œç¶´ï¼Œå¯ä»¥æ˜¯å­—ä¸²æˆ–åˆ—è¡¨ (é è¨­: ['.nii.gz', '.nii'])
            transform: é¡å¤–çš„è³‡æ–™å¢å¼·è®Šæ›ï¼ˆæœƒåœ¨å…§å»ºå¢å¼·ä¹‹å¾ŒåŸ·è¡Œï¼‰
            target_size: ç›®æ¨™å°ºå¯¸ (D, H, W)ï¼Œè‹¥ç‚º None å‰‡ä¸èª¿æ•´å¤§å°
            num_classes: é¡åˆ¥æ•¸é‡ï¼Œç”¨æ–¼æª¢æŸ¥æ¨™ç±¤ç¯„åœ
            debug_labels: æ˜¯å¦å•Ÿç”¨æ¨™ç±¤é™¤éŒ¯æ¨¡å¼
            use_augmentation: æ˜¯å¦ä½¿ç”¨æ•¸æ“šå¢å¼·ï¼ˆåªå°è¨“ç·´é›†æœ‰æ•ˆï¼‰
            augmentation_type: æ•¸æ“šå¢å¼·é¡å‹ ('light', 'medium', 'heavy', 'medical', 'medical_heavy')
        """
        self.data_root = Path(data_root)
        self.split = split
        self.external_transform = transform
        self.target_size = target_size
        self.num_classes = num_classes
        self.debug_labels = debug_labels
        
        # æ•¸æ“šå¢å¼·è¨­å®šï¼šåªå°è¨“ç·´é›†å•Ÿç”¨
        self.use_augmentation = use_augmentation and (split == 'train')
        if self.use_augmentation:
            if augmentation_type == 'light':
                self.augmentation_transform = get_light_augmentation()
            elif augmentation_type == 'medium':
                self.augmentation_transform = get_medium_augmentation()
            elif augmentation_type == 'heavy':
                self.augmentation_transform = get_heavy_augmentation()
            elif augmentation_type == 'medical':
                self.augmentation_transform = get_medical_augmentation()
            elif augmentation_type == 'medical_heavy':
                self.augmentation_transform = get_medical_artifact_heavy()
            elif augmentation_type == 'custom':
                self.augmentation_transform = get_custom_augmentation()
            else:
                print(f"âš ï¸ æœªçŸ¥çš„å¢å¼·é¡å‹: {augmentation_type}, ä½¿ç”¨é è¨­ medical")
                self.augmentation_transform = get_medical_augmentation()
            
            print(f"è¨“ç·´é›†å•Ÿç”¨ {augmentation_type} æ•¸æ“šå¢å¼·")
        else:
            self.augmentation_transform = None
            if split == 'train':
                print("è¨“ç·´é›†æœªå•Ÿç”¨æ•¸æ“šå¢å¼·")
            else:
                print(f"{split.upper()} é›†æœªå•Ÿç”¨æ•¸æ“šå¢å¼·ï¼ˆæ­£ç¢ºï¼‰")
        
        # è¨­å®šå½±åƒå’Œæ¨™ç±¤ç›®éŒ„
        self.image_dir = self.data_root / split / 'images'
        self.mask_dir = self.data_root / split / 'labels'
        
        # æª¢æŸ¥ç›®éŒ„æ˜¯å¦å­˜åœ¨
        if not self.image_dir.exists():
            raise FileNotFoundError(f"å½±åƒç›®éŒ„ä¸å­˜åœ¨: {self.image_dir}")
        if not self.mask_dir.exists():
            raise FileNotFoundError(f"æ¨™ç±¤ç›®éŒ„ä¸å­˜åœ¨: {self.mask_dir}")
        
        # ç¢ºä¿ suffix æ˜¯åˆ—è¡¨æ ¼å¼
        if isinstance(image_suffix, str):
            image_suffix = [image_suffix]
        if isinstance(mask_suffix, str):
            mask_suffix = [mask_suffix]
        
        # å°‹æ‰¾é…å°çš„æª”æ¡ˆï¼ˆæ”¯æ´å¤šç¨®å¾Œç¶´ï¼‰
        self.image_files = []
        for suffix in image_suffix:
            self.image_files.extend(list(self.image_dir.glob(f'*{suffix}')))
        
        # å»é™¤é‡è¤‡æª”æ¡ˆ
        self.image_files = list(set(self.image_files))
        
        self.pairs = []
        
        for img_file in self.image_files:
            # å–å¾—æª”åï¼ˆä¸å«å‰¯æª”åï¼‰
            img_stem = img_file.stem
            if img_stem.endswith('.nii'):  # è™•ç† .nii.gz çš„æƒ…æ³
                img_stem = img_stem[:-4]
            
            # å˜—è©¦å°‹æ‰¾å°æ‡‰çš„æ¨™ç±¤æª”æ¡ˆï¼ˆå˜—è©¦æ‰€æœ‰å¯èƒ½çš„å¾Œç¶´ï¼‰
            mask_found = False
            for suffix in mask_suffix:
                mask_file = self.mask_dir / f"{img_stem}{suffix}"
                if mask_file.exists():
                    self.pairs.append((img_file, mask_file))
                    mask_found = True
                    break
            
            if not mask_found:
                print(f"è­¦å‘Š: æ‰¾ä¸åˆ°å°æ‡‰çš„æ¨™ç±¤æª”æ¡ˆ {img_stem}")
        
        print(f"[{split.upper()}] æ‰¾åˆ° {len(self.pairs)} å°è³‡æ–™ (æ”¯æ´ .nii å’Œ .nii.gz)")
        
        # åˆ†ææ•´å€‹è³‡æ–™é›†çš„æ¨™ç±¤åˆ†ä½ˆï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è¼‰å…¥æ™‚åŸ·è¡Œï¼‰
        if self.debug_labels and len(self.pairs) > 0:
            self.analyze_label_distribution()
    
    def analyze_label_distribution(self):
        """åˆ†ææ•´å€‹è³‡æ–™é›†çš„æ¨™ç±¤åˆ†ä½ˆ"""
        #print(f"\n=== [{self.split.upper()}] æ¨™ç±¤åˆ†ä½ˆåˆ†æ ===")
        all_unique_values = set()
        problematic_files = []
        
        # åˆ†æå‰5å€‹æª”æ¡ˆçš„æ¨™ç±¤åˆ†ä½ˆ
        sample_size = min(5, len(self.pairs))
        for i in range(sample_size):
            _, mask_path = self.pairs[i]
            try:
                mask = self.load_nii_image(mask_path)
                unique_vals = np.unique(mask)
                all_unique_values.update(unique_vals)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ç•°å¸¸å€¼
                max_val = np.max(unique_vals)
                min_val = np.min(unique_vals)
                
                if min_val < 0 or (self.num_classes and max_val >= self.num_classes):
                    problematic_files.append((mask_path.name, unique_vals))
                
                #print(f"æª”æ¡ˆ {mask_path.name}: æ¨™ç±¤å€¼ç¯„åœ [{min_val:.1f}, {max_val:.1f}], å”¯ä¸€å€¼: {len(unique_vals)}")
                
            except Exception as e:
                print(f"åˆ†ææª”æ¡ˆ {mask_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # print(f"æ‰€æœ‰å”¯ä¸€æ¨™ç±¤å€¼: {sorted(list(all_unique_values))}")
        
        # if self.num_classes:
        #     print(f"é æœŸé¡åˆ¥æ•¸: {self.num_classes} (ç¯„åœ: 0 åˆ° {self.num_classes-1})")
        #     invalid_values = [v for v in all_unique_values if v < 0 or v >= self.num_classes]
        #     if invalid_values:
        #         print(f"âš ï¸  ç™¼ç¾ç„¡æ•ˆæ¨™ç±¤å€¼: {invalid_values}")
                
        # if problematic_files:
        #     #print(f"âš ï¸  ç™¼ç¾ {len(problematic_files)} å€‹å•é¡Œæª”æ¡ˆ:")
        #     for filename, vals in problematic_files:
        #         print(f"   - {filename}: {vals}")
        
        print("=== åˆ†æå®Œæˆ ===\n")
    
    def __len__(self):
        return len(self.pairs)
    
    def load_nii_image(self, file_path):
        """è¼‰å…¥ NII.GZ æª”æ¡ˆï¼Œæ”¯æ´3Då’Œ4Dåœ–åƒ"""
        try:
            nii_img = nib.load(str(file_path))
            img_data = nii_img.get_fdata()
            
            # è½‰æ›ç‚º numpy array ä¸¦ç¢ºä¿æ˜¯ float32 é¡å‹
            img_data = np.array(img_data, dtype=np.float32)
            
            # æª¢æŸ¥åœ–åƒç¶­åº¦ä¸¦è™•ç†4Dæƒ…æ³
            if img_data.ndim == 4:
                # å°æ–¼4Dåœ–åƒï¼Œå–ç¬¬ä¸€å€‹æ™‚é–“é»æˆ–æœ€å¾Œä¸€å€‹ç¶­åº¦çš„ç¬¬ä¸€å€‹åˆ‡ç‰‡
                #print(f"æª¢æ¸¬åˆ°4Dåœ–åƒ {file_path.name}ï¼Œå½¢ç‹€: {img_data.shape}")
                
                # å¸¸è¦‹çš„4Dé†«å­¸å½±åƒæ ¼å¼ï¼š(x, y, z, time) æˆ– (x, y, z, channel)
                # é€šå¸¸æˆ‘å€‘å–ç¬¬ä¸€å€‹æ™‚é–“é»æˆ–é€šé“
                img_data = img_data[:, :, :, 0]
                #print(f"è½‰æ›ç‚º3Dåœ–åƒï¼Œæ–°å½¢ç‹€: {img_data.shape}")
                
            elif img_data.ndim < 3:
                raise ValueError(f"åœ–åƒç¶­åº¦éä½: {img_data.ndim}Dï¼Œéœ€è¦è‡³å°‘3D")
            elif img_data.ndim > 4:
                raise ValueError(f"ä¸æ”¯æ´çš„åœ–åƒç¶­åº¦: {img_data.ndim}Dï¼Œæ”¯æ´3Dæˆ–4D")
            
            return img_data
        except Exception as e:
            print(f"è®€å–æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise
    
    def normalize_image(self, image):
        """å½±åƒæ¨™æº–åŒ–"""
        # ç§»é™¤ç•°å¸¸å€¼ï¼ˆå¯é¸ï¼‰
        p1, p99 = np.percentile(image, (1, 99))
        image = np.clip(image, p1, p99)
        
        # Z-score æ¨™æº–åŒ–
        mean = np.mean(image)
        std = np.std(image)
        if std > 0:
            image = (image - mean) / std
        
        return image
    
    def resize_volume(self, volume, target_size):
        """èª¿æ•´ 3D é«”ç©å¤§å°ï¼Œæ”¯æ´ä¸åŒè¼¸å…¥ç¶­åº¦"""
        if target_size is None:
            return volume
            
        from scipy.ndimage import zoom
        
        # ç¢ºä¿è¼¸å…¥æ˜¯3Dçš„
        if volume.ndim != 3:
            raise ValueError(f"resize_volume åªæ”¯æ´3Dé«”ç©ï¼Œæ”¶åˆ° {volume.ndim}D")
        
        current_size = volume.shape
        
        # ç¢ºä¿ target_size ä¹Ÿæ˜¯3Dçš„
        if len(target_size) != 3:
            raise ValueError(f"target_size å¿…é ˆæ˜¯3D (D, H, W)ï¼Œæ”¶åˆ° {len(target_size)}D")
        
        zoom_factors = [t/c for t, c in zip(target_size, current_size)]
        
        # å°æ–¼æ¨™ç±¤ä½¿ç”¨æœ€è¿‘é„°æ’å€¼ï¼Œå°æ–¼å½±åƒä½¿ç”¨ç·šæ€§æ’å€¼
        if len(np.unique(volume)) < 10:  # å‡è¨­æ˜¯æ¨™ç±¤
            resized = zoom(volume, zoom_factors, order=0)
        else:  # å‡è¨­æ˜¯å½±åƒ
            resized = zoom(volume, zoom_factors, order=1)
            
        return resized
    
    def clean_labels(self, mask, file_path=None):
        """æ¸…ç†æ¨™ç±¤è³‡æ–™ï¼Œåªä¿ç•™0å’Œ1 - ç°¡åŒ–ç‰ˆæœ¬"""
        # ç¢ºä¿maskæ˜¯3Dçš„
        if mask.ndim != 3:
            print(f"è­¦å‘Š: æ¨™ç±¤ç¶­åº¦ç•°å¸¸ ({mask.ndim}D)ï¼Œé æœŸ3D")
            if mask.ndim == 4:
                # å¦‚æœæ˜¯4Dæ¨™ç±¤ï¼Œå–ç¬¬ä¸€å€‹é€šé“/æ™‚é–“é»
                mask = mask[:, :, :, 0]
                print(f"4Dæ¨™ç±¤å·²è½‰æ›ç‚º3D")
        
        # ç§»é™¤ NaN å€¼å’Œç„¡çª®å¤§å€¼
        mask = np.nan_to_num(mask, nan=0.0, posinf=0.0, neginf=0.0)
        
        # ç°¡å–®ä½†æœ‰æ•ˆçš„äºŒå…ƒåŒ–ï¼šæ‰€æœ‰éé›¶å€¼éƒ½è®Šæˆ1
        mask[mask > 0] = 1
        
        # ç¢ºä¿æ˜¯æ•´æ•¸é¡å‹
        mask = mask.astype(np.int64)
        
        # ç°¡å–®é©—è­‰
        unique_vals = np.unique(mask)
        #if self.debug_labels and file_path:
            #print(f"ğŸ“‹ æ¨™ç±¤è™•ç† {Path(file_path).name}: {unique_vals}")
        
        # æœ€çµ‚æª¢æŸ¥
        if not set(unique_vals).issubset({0, 1}):
            print(f"âš ï¸ ç™¼ç¾ç•°å¸¸æ¨™ç±¤å€¼: {unique_vals}")
            # å†æ¬¡å¼·åˆ¶äºŒå…ƒåŒ–
            mask = (mask > 0).astype(np.int64)
        
        return mask
    
    def handle_dimension_mismatch(self, image, mask, file_path=None):
        """è™•ç†å½±åƒå’Œæ¨™ç±¤ç¶­åº¦ä¸åŒ¹é…çš„å•é¡Œ"""
        if image.shape != mask.shape:
            print(f"ç¶­åº¦ä¸åŒ¹é… - å½±åƒ: {image.shape}, æ¨™ç±¤: {mask.shape}")
            
            # å¦‚æœåªæ˜¯å…¶ä¸­ä¸€å€‹æ˜¯4Dï¼Œå…ˆé™ç¶­
            if image.ndim == 4 and mask.ndim == 3:
                print("å½±åƒæ˜¯4Dï¼Œæ¨™ç±¤æ˜¯3D - å°‡å½±åƒè½‰ç‚º3D")
                image = image[:, :, :, 0]
            elif image.ndim == 3 and mask.ndim == 4:
                print("å½±åƒæ˜¯3Dï¼Œæ¨™ç±¤æ˜¯4D - å°‡æ¨™ç±¤è½‰ç‚º3D") 
                mask = mask[:, :, :, 0]
            
            # å†æ¬¡æª¢æŸ¥ç¶­åº¦
            if image.shape != mask.shape:
                print(f"èª¿æ•´å¾Œç¶­åº¦ä»ä¸åŒ¹é… - å½±åƒ: {image.shape}, æ¨™ç±¤: {mask.shape}")
                # å¯ä»¥é¸æ“‡é€²ä¸€æ­¥çš„è™•ç†ç­–ç•¥ï¼Œä¾‹å¦‚è£å‰ªæˆ–å¡«å……
                min_shape = [min(i, m) for i, m in zip(image.shape, mask.shape)]
                print(f"å°‡å…©è€…è£å‰ªåˆ°å…±åŒæœ€å°å°ºå¯¸: {min_shape}")
                
                image = image[:min_shape[0], :min_shape[1], :min_shape[2]]
                mask = mask[:min_shape[0], :min_shape[1], :min_shape[2]]
        
        return image, mask
    
    def __getitem__(self, idx):
        img_path, mask_path = self.pairs[idx]
        
        try:
            # è¼‰å…¥å½±åƒå’Œæ¨™ç±¤
            image = self.load_nii_image(img_path)
            mask = self.load_nii_image(mask_path)
            
            # è™•ç†ç¶­åº¦ä¸åŒ¹é…å•é¡Œ
            image, mask = self.handle_dimension_mismatch(image, mask, img_path.name)
            
            # èª¿æ•´å¤§å°ï¼ˆç¢ºä¿éƒ½æ˜¯3Då¾Œæ‰èª¿æ•´ï¼‰
            if self.target_size is not None:
                image = self.resize_volume(image, self.target_size)
                mask = self.resize_volume(mask, self.target_size)
            
            # æ¨™æº–åŒ–å½±åƒ
            image = self.normalize_image(image)
            
            # æ¸…ç†å’Œé©—è­‰æ¨™ç±¤
            mask = self.clean_labels(mask, mask_path.name)
            
            # æ·»åŠ é€šé“ç¶­åº¦ï¼š[D, H, W] -> [1, D, H, W]
            image = image[np.newaxis, ...]
            
            # è½‰æ›ç‚º tensor
            image = torch.from_numpy(image).float()
            mask = torch.from_numpy(mask).long()
            
            # æœ€çµ‚é©—è­‰ tensorï¼šç¢ºä¿åªæœ‰0å’Œ1
            mask_unique = torch.unique(mask)
            if not all(val in [0, 1] for val in mask_unique.tolist()):
                raise ValueError(f"Tensor åŒ…å«éäºŒå…ƒæ¨™ç±¤å€¼: {mask_unique.tolist()}, é æœŸåªæœ‰ [0, 1]")
            
            # æ‡‰ç”¨å…§å»ºæ•¸æ“šå¢å¼·ï¼ˆåªå°è¨“ç·´é›†ï¼‰
            if self.augmentation_transform is not None:
                image, mask = self.augmentation_transform(image, mask)
            
            # æ‡‰ç”¨é¡å¤–çš„å¤–éƒ¨è®Šæ›
            if self.external_transform is not None:
                image, mask = self.external_transform(image, mask)
            
            return {
                'image': image,
                'mask': mask,
                'image_path': str(img_path),
                'mask_path': str(mask_path)
            }
            
        except Exception as e:
            print(f"âŒ è™•ç†æª”æ¡ˆå° {img_path.name} / {mask_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print(f"è©³ç´°éŒ¯èª¤è³‡è¨Š: {type(e).__name__}: {str(e)}")
            raise


# ==================== è³‡æ–™è¼‰å…¥å·¥å…·å‡½æ•¸ ====================
def create_data_loaders(data_root, batch_size=2, target_size=(64, 64, 64), 
                       num_workers=2, use_augmentation=True, augmentation_type='medical'):
    """
    å‰µå»ºè¨“ç·´ã€é©—è­‰å’Œæ¸¬è©¦è³‡æ–™è¼‰å…¥å™¨
    æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬å¼·åˆ¶åŸ·è¡ŒäºŒå…ƒåˆ†é¡ï¼Œåªä¿ç•™æ¨™ç±¤0å’Œ1ï¼Œä¸¦ä¸”åªå°è¨“ç·´é›†é€²è¡Œæ•¸æ“šå¢å¼·
    æ”¯æ´3Då’Œ4Dé†«å­¸å½±åƒæ ¼å¼
    
    Args:
        data_root: è³‡æ–™æ ¹ç›®éŒ„
        batch_size: æ‰¹æ¬¡å¤§å°
        target_size: ç›®æ¨™å°ºå¯¸ (D, H, W)
        num_workers: è³‡æ–™è¼‰å…¥åŸ·è¡Œç·’æ•¸
        use_augmentation: æ˜¯å¦å°è¨“ç·´é›†ä½¿ç”¨æ•¸æ“šå¢å¼·
        augmentation_type: æ•¸æ“šå¢å¼·é¡å‹ ('light', 'medium', 'heavy', 'medical', 'medical_heavy')
    """
    data_loaders = {}
    
    for split in ['train', 'val', 'test']:
        split_path = Path(data_root) / split
        if split_path.exists():
            # åªåœ¨è¨“ç·´é›†å•Ÿç”¨è©³ç´°é™¤éŒ¯
            debug_mode = (split == 'train')
            
            dataset = MedicalImageDataset(
                data_root=data_root,
                split=split,
                target_size=target_size,
                num_classes=2,  # å›ºå®šç‚ºäºŒå…ƒåˆ†é¡
                debug_labels=debug_mode,
                use_augmentation=use_augmentation,
                augmentation_type=augmentation_type
            )
            
            shuffle = True if split == 'train' else False
            
            data_loaders[split] = DataLoader(
                dataset, 
                batch_size=batch_size, 
                shuffle=shuffle, 
                num_workers=num_workers,
                pin_memory=True
            )
        else:
            print(f"è­¦å‘Š: {split} ç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³é")
    
    return data_loaders


# ==================== é™¤éŒ¯å·¥å…·å‡½æ•¸ ====================
def debug_data_loader(data_loader, num_samples=3):
    """
    é™¤éŒ¯è³‡æ–™è¼‰å…¥å™¨ï¼Œæª¢æŸ¥å¹¾å€‹æ‰¹æ¬¡çš„è³‡æ–™
    æ­¤ç‰ˆæœ¬å°ˆç‚ºäºŒå…ƒåˆ†é¡è¨­è¨ˆï¼ŒåŒ…å«æ•¸æ“šå¢å¼·è³‡è¨Šï¼Œæ”¯æ´3D/4Dåœ–åƒ
    """
    print("\n=== DataLoader äºŒå…ƒåˆ†é¡é™¤éŒ¯è³‡è¨Š (æ”¯æ´3D/4D) ===")
    print(f"æ‰¹æ¬¡å¤§å°: {data_loader.batch_size}")
    print(f"è³‡æ–™é›†å¤§å°: {len(data_loader.dataset)}")
    print(f"æ‰¹æ¬¡æ•¸é‡: {len(data_loader)}")
    print("æ¨™ç±¤èªªæ˜: 0=èƒŒæ™¯, 1=å‰æ™¯")
    print("ç¶­åº¦æ”¯æ´: 3Då’Œ4Dé†«å­¸å½±åƒï¼ˆ4Dæœƒè‡ªå‹•è½‰æ›ç‚º3Dï¼‰")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰æ•¸æ“šå¢å¼·
    if hasattr(data_loader.dataset, 'augmentation_transform') and data_loader.dataset.augmentation_transform:
        print("ğŸ”„ æ•¸æ“šå¢å¼·: å·²å•Ÿç”¨")
        print(f"å¢å¼·é¡å‹: {len(data_loader.dataset.augmentation_transform.transforms)} ç¨®è®Šæ›")
    else:
        print("âŒ æ•¸æ“šå¢å¼·: æœªå•Ÿç”¨")
    
    for i, batch in enumerate(data_loader):
        if i >= num_samples:
            break
            
        images = batch['image']
        masks = batch['mask']
        
        print(f"\n--- æ‰¹æ¬¡ {i+1} ---")
        print(f"å½±åƒ shape: {images.shape}, dtype: {images.dtype}")
        print(f"æ¨™ç±¤ shape: {masks.shape}, dtype: {masks.dtype}")
        print(f"å½±åƒå€¼ç¯„åœ: [{images.min():.3f}, {images.max():.3f}]")
        print(f"æ¨™ç±¤å€¼ç¯„åœ: [{masks.min()}, {masks.max()}]")
        print(f"æ¨™ç±¤å”¯ä¸€å€¼: {torch.unique(masks).tolist()}")
        
        # æª¢æŸ¥äºŒå…ƒæ¨™ç±¤
        unique_labels = torch.unique(masks).tolist()
        if set(unique_labels).issubset({0, 1}):
            print("âœ… æ¨™ç±¤æ­£ç¢ºï¼šåªåŒ…å«0å’Œ1")
            
            # çµ±è¨ˆåƒç´ åˆ†ä½ˆ
            total_pixels = masks.numel()
            background_pixels = (masks == 0).sum().item()
            foreground_pixels = (masks == 1).sum().item()
            
            bg_percentage = (background_pixels / total_pixels) * 100
            fg_percentage = (foreground_pixels / total_pixels) * 100
            
            print(f"   èƒŒæ™¯(0): {background_pixels} åƒç´  ({bg_percentage:.2f}%)")
            print(f"   å‰æ™¯(1): {foreground_pixels} åƒç´  ({fg_percentage:.2f}%)")
        else:
            print(f"âŒ æ¨™ç±¤éŒ¯èª¤ï¼šåŒ…å«éäºŒå…ƒå€¼ {unique_labels}")
    
    print("=== é™¤éŒ¯å®Œæˆ ===\n")


# ==================== ä½¿ç”¨ç¯„ä¾‹å’Œæ¸¬è©¦å‡½æ•¸ ====================
def test_augmented_dataset():
    """æ¸¬è©¦æ•´åˆæ•¸æ“šå¢å¼·çš„è³‡æ–™é›†ï¼ˆæ”¯æ´3D/4Dåœ–åƒï¼‰"""
    # å‡è¨­æ‚¨çš„è³‡æ–™è·¯å¾‘
    data_root = r"D:\unet3d\dataset"
    
    print("ğŸ§ª é–‹å§‹æ¸¬è©¦ä¸åŒç­‰ç´šçš„æ•¸æ“šå¢å¼· (æ”¯æ´3D/4Dåœ–åƒ)...")
    
    augmentation_types = ['light', 'medium', 'heavy', 'medical', 'medical_heavy']
    
    for aug_type in augmentation_types:
        print(f"\n{'='*50}")
        print(f"ğŸ”„ æ¸¬è©¦ {aug_type.upper()} æ•¸æ“šå¢å¼·")
        print(f"{'='*50}")
        
        try:
            # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
            data_loaders = create_data_loaders(
                data_root=data_root,
                batch_size=1,  # ä½¿ç”¨å°æ‰¹æ¬¡é€²è¡Œæ¸¬è©¦
                target_size=(32, 32, 32),  # ä½¿ç”¨å°å°ºå¯¸é€²è¡Œå¿«é€Ÿæ¸¬è©¦
                use_augmentation=True,
                augmentation_type=aug_type
            )
            
            # æ¸¬è©¦è¨“ç·´é›†è³‡æ–™è¼‰å…¥å™¨
            if 'train' in data_loaders:
                print(f"\nğŸ“Š {aug_type} å¢å¼·æ•ˆæœæ¸¬è©¦:")
                debug_data_loader(data_loaders['train'], num_samples=1)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°è¨“ç·´é›†è³‡æ–™")
                
        except FileNotFoundError as e:
            print(f"âŒ è³‡æ–™è·¯å¾‘éŒ¯èª¤: {e}")
            print("è«‹ç¢ºèªæ‚¨çš„è³‡æ–™ç›®éŒ„çµæ§‹å¦‚ä¸‹:")
            print("data_root/")
            print("â”œâ”€â”€ train/")
            print("â”‚   â”œâ”€â”€ images/")
            print("â”‚   â””â”€â”€ labels/")
            print("â”œâ”€â”€ val/")
            print("â”‚   â”œâ”€â”€ images/")
            print("â”‚   â””â”€â”€ labels/")
            print("â””â”€â”€ test/")
            print("    â”œâ”€â”€ images/")
            print("    â””â”€â”€ labels/")
            break
        except Exception as e:
            print(f"âŒ æ¸¬è©¦ {aug_type} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


