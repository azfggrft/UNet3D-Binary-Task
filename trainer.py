
import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import argparse
from pathlib import Path
import random
import numpy as np
import time
import json
import warnings
warnings.filterwarnings('ignore')

# é€²åº¦æ¢åº«
try:
    from tqdm import tqdm, trange
    TQDM_AVAILABLE = True
except ImportError:
    print("å»ºè­°å®‰è£ tqdm ä»¥ç²å¾—æ›´å¥½çš„é€²åº¦æ¢é«”é©—: pip install tqdm")
    TQDM_AVAILABLE = False

from src.network_architecture.net_module import *
from src.network_architecture.unet3d import UNet3D
from src.loss_architecture.loss import DiceLoss, CombinedLoss
from src.loss_architecture.calculate_dice import calculate_dice_score, calculate_metrics
from src.data_processing_and_data_enhancement.visualizer import UNet3DVisualizer

class EnhancedUNet3DTrainer:
    """ä¿®å¾© PyTorch 2.6 è¼‰å…¥å•é¡Œçš„å¢å¼·ç‰ˆ 3D UNet è¨“ç·´ç®¡ç†å™¨"""
    
    def __init__(self, model, data_loaders, optimizer, criterion, device, 
                 save_dir='./checkpoints', log_interval=10, save_interval=10,
                 scheduler=None, visualize=True, plot_interval=10, 
                 use_progress_bar=True, training_config=None):  # æ–°å¢åƒæ•¸
        
        self.model = model.to(device) if model is not None else None
        self.data_loaders = data_loaders
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device if device is not None else torch.device('cpu')
        self.scheduler = scheduler
        self.save_dir = Path(save_dir) if save_dir is not None else Path('./checkpoints')
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.log_interval = log_interval
        self.save_interval = save_interval
        self.visualize = visualize
        self.plot_interval = plot_interval
        self.use_progress_bar = use_progress_bar and TQDM_AVAILABLE
        self.training_start_time = None
        self.training_end_time = None
        self.total_training_time = 0
        
        # ä¿å­˜è¨“ç·´é…ç½®ï¼ˆæ–°å¢ï¼‰
        self.training_config = training_config if training_config is not None else {}
        
        # è¨ˆç®—æ¨¡å‹è¤‡é›œåº¦è³‡è¨Š
        self.model_info = self._calculate_model_complexity()
        
        # åˆå§‹åŒ–è¦–è¦ºåŒ–å™¨
        if self.visualize:
            viz_dir = self.save_dir / 'visualizations'
            self.visualizer = UNet3DVisualizer(save_dir=viz_dir)
            print(f"è¦–è¦ºåŒ–åŠŸèƒ½å·²å•Ÿç”¨ï¼Œåœ–ç‰‡ä¿å­˜è‡³: {viz_dir}")
        
        # è¨“ç·´æ­·å²è¨˜éŒ„
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_dice': [],
            'val_dice': [],
            'learning_rate': [],
            'epoch_time': [],
            'gpu_memory': []
        }
        
        # æœ€ä½³æ¨¡å‹è¿½è¹¤
        self.best_val_dice = 0.0
        self.best_epoch = 0
        
        self.start_time = None
        self.epoch_start_time = None
        
        # é€²åº¦æ¢ç›¸é—œ
        self.epoch_pbar = None
        self.train_pbar = None
        self.val_pbar = None

    def _calculate_model_complexity(self):
        """è¨ˆç®—æ¨¡å‹è¤‡é›œåº¦è³‡è¨Š"""
        try:
            # ç²å–æ¨¡å‹åƒæ•¸æ•¸é‡
            if hasattr(self.model, 'get_model_size'):
                total_params, trainable_params = self.model.get_model_size()
            else:
                total_params = sum(p.numel() for p in self.model.parameters())
                trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            # å˜—è©¦è¨ˆç®—FLOPsï¼ˆå¯é¸ï¼‰
            try:
                from thop import profile, clever_format
                
                # å‰µå»ºä¸€å€‹ç¤ºä¾‹è¼¸å…¥
                if hasattr(self.model, 'n_channels'):
                    n_channels = self.model.n_channels
                else:
                    n_channels = 1
                
                # ä½¿ç”¨å¯¦éš›çš„è¨“ç·´å°ºå¯¸ä¾†è¨ˆç®—FLOPs
                target_size = self.training_config.get('target_size', (64, 64, 64))
                sample_input = torch.randn(1, n_channels, *target_size).to(self.device)
                
                # è¨ˆç®—FLOPs
                flops, params = profile(self.model, inputs=(sample_input,), verbose=False)
                flops = flops / 2 / 1e9
                flops_str, params_str = clever_format([flops, params], "%.3f")
                
                # æ¸¬è©¦æ¨ç†æ™‚é–“
                self.model.eval()
                inference_times = []
                
                with torch.no_grad():
                    # æš–èº«é‹è¡Œ
                    for _ in range(3):
                        _ = self.model(sample_input)
                    
                    # å¯¦éš›æ¸¬é‡
                    for _ in range(10):
                        start_time = time.time()
                        _ = self.model(sample_input)
                        torch.cuda.synchronize() if self.device.type == 'cuda' else None
                        end_time = time.time()
                        inference_times.append(end_time - start_time)
                
                avg_inference_time = np.mean(inference_times)
                
            except Exception as e:
                print(f"è¨ˆç®—FLOPsæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                flops_str = "ç„¡æ³•è¨ˆç®—"
                avg_inference_time = 0
            
            return {
                'total_params': total_params,
                'trainable_params': trainable_params,
                'model_size_mb': total_params * 4 / 1024 / 1024,  # FP32
                'flops': flops_str,
                'avg_inference_time': avg_inference_time
            }
            
        except Exception as e:
            print(f"è¨ˆç®—æ¨¡å‹è¤‡é›œåº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                'total_params': 'Unknown',
                'trainable_params': 'Unknown', 
                'model_size_mb': 0,
                'flops': 'Unknown',
                'avg_inference_time': 0
            }
    
    def safe_torch_load(self, path):
        """å®‰å…¨çš„ torch.load å‡½æ•¸ï¼Œå…¼å®¹ PyTorch 2.6+"""
        try:
            # é¦–å…ˆå˜—è©¦ä½¿ç”¨ weights_only=Falseï¼ˆé©ç”¨æ–¼ä¿¡ä»»çš„æª¢æŸ¥é»ï¼‰
            return torch.load(path, map_location=self.device, weights_only=False)
        except Exception as e1:
            try:
                # å¦‚æœå¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ weights_only=True
                print(f"ä½¿ç”¨ weights_only=False è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦ weights_only=True")
                return torch.load(path, map_location=self.device, weights_only=True)
            except Exception as e2:
                # æœ€å¾Œå˜—è©¦ä½¿ç”¨å®‰å…¨å…¨åŸŸè®Šæ•¸
                print(f"æ­£åœ¨è¨­ç½®å®‰å…¨å…¨åŸŸè®Šæ•¸ä¸¦é‡æ–°è¼‰å…¥...")
                torch.serialization.add_safe_globals([
                    'numpy._core.multiarray.scalar',
                    'numpy.core.multiarray.scalar',
                    'numpy.dtype',
                    'numpy.ndarray',
                    'collections.OrderedDict'
                ])
                return torch.load(path, map_location=self.device, weights_only=True)
    
    def log_gpu_memory(self):
        """è¨˜éŒ„GPUè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated(self.device) / 1024**3
            memory_cached = torch.cuda.memory_reserved(self.device) / 1024**3
            self.history['gpu_memory'].append({
                'allocated': memory_allocated,
                'cached': memory_cached
            })
        else:
            self.history['gpu_memory'].append({'allocated': 0, 'cached': 0})
    
    def _format_dice_score(self, dice_score):
        """
        å®‰å…¨åœ°æ ¼å¼åŒ– Dice åˆ†æ•¸ï¼Œè™•ç†åˆ—è¡¨å’Œå–®ä¸€æ•¸å€¼æƒ…æ³
        
        Args:
            dice_score: å¯èƒ½æ˜¯å–®ä¸€æ•¸å€¼æˆ–åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–å¾Œçš„å­—ç¬¦ä¸²
        """
        if isinstance(dice_score, (list, tuple)):
            if len(dice_score) == 1:
                # åªæœ‰ä¸€å€‹å‰æ™¯é¡åˆ¥çš„æƒ…æ³
                return f'{dice_score[0]:.4f}'
            else:
                # å¤šé¡åˆ¥çš„æƒ…æ³ï¼Œé¡¯ç¤ºå¹³å‡å€¼
                mean_dice = np.mean(dice_score)
                return f'{mean_dice:.4f}'
        else:
            # å–®ä¸€æ•¸å€¼çš„æƒ…æ³
            return f'{dice_score:.4f}'
    
    def train_one_epoch(self, epoch):
        """è¨“ç·´ä¸€å€‹ epoch - ä¿®å¾©å­¸ç¿’ç‡é¡¯ç¤º"""
        self.epoch_start_time = time.time()
        self.model.train()
        train_loss = 0.0
        train_dice_scores = []
        
        num_batches = len(self.data_loaders['train'])
        
        # æª¢æŸ¥æ˜¯å¦åœ¨ warmup éšæ®µ
        has_warmup = hasattr(self, 'warmup_scheduler') and self.warmup_scheduler is not None
        warmup_epochs = getattr(self, 'warmup_epochs', 0) if has_warmup else 0
        is_warmup_stage = has_warmup and epoch < warmup_epochs
        
        # å‰µå»ºè¨“ç·´é€²åº¦æ¢
        if self.use_progress_bar:
            desc_prefix = "ğŸ”¥ Warmup" if is_warmup_stage else "ğŸš€ Train"
            self.train_pbar = tqdm(
                enumerate(self.data_loaders['train']), 
                total=num_batches,
                desc=f"{desc_prefix} E{epoch+1:3d}",
                leave=False,
                ncols=100,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}'
            )
        else:
            stage_info = "Warmup è¨“ç·´" if is_warmup_stage else "æ­£å¸¸è¨“ç·´"
            print(f"\né–‹å§‹{stage_info} Epoch {epoch+1}")
        
        data_iter = self.train_pbar if self.use_progress_bar else enumerate(self.data_loaders['train'])
        
        for batch_idx, batch in data_iter:
            images = batch['image'].to(self.device)
            masks = batch['mask'].to(self.device)
            
            # ç¢ºä¿æ¨™ç±¤ç‚ºäºŒå…ƒåˆ†é¡
            masks[masks > 0] = 1
            
            self.optimizer.zero_grad()
            outputs = self.model(images)
            
            loss = self.criterion(outputs, masks)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            train_loss += loss.item()
            
            with torch.no_grad():
                dice_score = calculate_dice_score(
                    outputs, masks, num_classes=self.model.n_classes
                )
                train_dice_scores.append(dice_score)
            
            # æ›´æ–°é€²åº¦æ¢ - é¡¯ç¤ºå¯¦éš›ç•¶å‰å­¸ç¿’ç‡
            if self.use_progress_bar:
                current_lr = self.optimizer.param_groups[0]['lr']  # ç²å–å¯¦éš›çš„å­¸ç¿’ç‡
                
                # ç²å–å‹•é‡è³‡è¨Š
                momentum_info = ""
                if 'momentum' in self.optimizer.param_groups[0]:
                    momentum = self.optimizer.param_groups[0]['momentum']
                    momentum_info = f", M:{momentum:.2f}"
                elif 'betas' in self.optimizer.param_groups[0]:
                    beta1 = self.optimizer.param_groups[0]['betas'][0]
                    momentum_info = f", Î²1:{beta1:.2f}"
                
                postfix_data = {
                    'Loss': f'{loss.item():.4f}',
                    'Dice': self._format_dice_score(dice_score),
                    'LR': f'{current_lr:.2e}'  # ä½¿ç”¨ç§‘å­¸è¨ˆæ•¸æ³•ç¯€çœç©ºé–“
                }
                
                # å¦‚æœæ˜¯ warmup éšæ®µï¼Œæ·»åŠ éšæ®µæ¨™è­˜
                if is_warmup_stage:
                    postfix_data['Stage'] = f'W{epoch+1}/{warmup_epochs}'
                    
                self.train_pbar.set_postfix(postfix_data)
                
            elif batch_idx % 5 == 0:
                progress = 100.0 * batch_idx / num_batches
                current_lr = self.optimizer.param_groups[0]['lr']
                dice_str = self._format_dice_score(dice_score)
                
                stage_prefix = "[WARMUP]" if is_warmup_stage else "[TRAIN]"
                print(f'\r  {stage_prefix} æ‰¹æ¬¡ [{batch_idx:3d}/{num_batches}] {progress:5.1f}% | '
                    f'æå¤±: {loss.item():.4f} | Dice: {dice_str} | LR: {current_lr:.6f}', end='')
        
        # é—œé–‰è¨“ç·´é€²åº¦æ¢
        if self.use_progress_bar and self.train_pbar:
            self.train_pbar.close()
        
        avg_train_loss = train_loss / num_batches
        
        # è¨ˆç®—å¹³å‡ Dice åˆ†æ•¸ - è™•ç†åˆ—è¡¨æ ¼å¼
        if train_dice_scores:
            if isinstance(train_dice_scores[0], (list, tuple)):
                avg_train_dice = np.mean([score[0] if len(score) > 0 else 0.0 for score in train_dice_scores])
            else:
                avg_train_dice = np.mean(train_dice_scores)
        else:
            avg_train_dice = 0.0
        
        epoch_time = time.time() - self.epoch_start_time
        self.history['epoch_time'].append(epoch_time)
        self.log_gpu_memory()
        
        if not self.use_progress_bar:
            stage_info = "Warmup è¨“ç·´" if is_warmup_stage else "è¨“ç·´"
            current_lr = self.optimizer.param_groups[0]['lr']
            print(f'\n  {stage_info}å®Œæˆ | å¹³å‡æå¤±: {avg_train_loss:.4f} | å¹³å‡Dice: {avg_train_dice:.4f} | '
                f'ç•¶å‰LR: {current_lr:.6f} | è€—æ™‚: {epoch_time:.1f}s')
        
        return avg_train_loss, avg_train_dice
    
    def validate_one_epoch(self, epoch):
        """é©—è­‰ä¸€å€‹ epoch"""
        if 'val' not in self.data_loaders:
            return 0.0, 0.0
            
        self.model.eval()
        val_loss = 0.0
        val_dice_scores = []
        
        num_batches = len(self.data_loaders['val'])
        
        # å‰µå»ºé©—è­‰é€²åº¦æ¢
        if self.use_progress_bar:
            self.val_pbar = tqdm(
                enumerate(self.data_loaders['val']), 
                total=num_batches,
                desc=f"Epoch {epoch+1:3d} é©—è­‰",
                leave=False,
                ncols=100,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {postfix}'
            )
        else:
            print(f"é–‹å§‹é©—è­‰...")
        
        data_iter = self.val_pbar if self.use_progress_bar else enumerate(self.data_loaders['val'])
        
        with torch.no_grad():
            for batch_idx, batch in data_iter:
                images = batch['image'].to(self.device)
                masks = batch['mask'].to(self.device)
                
                masks[masks > 0] = 1
                
                outputs = self.model(images)
                loss = self.criterion(outputs, masks)
                val_loss += loss.item()
                
                dice_score = calculate_dice_score(
                    outputs, masks, num_classes=self.model.n_classes
                )
                val_dice_scores.append(dice_score)
                
                # æ›´æ–°é€²åº¦æ¢ - ä¿®å¾©æ ¼å¼åŒ–å•é¡Œ
                if self.use_progress_bar:
                    self.val_pbar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Dice': self._format_dice_score(dice_score)  # ä½¿ç”¨å®‰å…¨çš„æ ¼å¼åŒ–å‡½æ•¸
                    })
                elif batch_idx % 3 == 0:
                    progress = 100.0 * batch_idx / num_batches
                    dice_str = self._format_dice_score(dice_score)  # ä½¿ç”¨å®‰å…¨çš„æ ¼å¼åŒ–å‡½æ•¸
                    print(f'\r  é©—è­‰é€²åº¦ {progress:5.1f}% | ç•¶å‰Dice: {dice_str}', end='')
        
        # é—œé–‰é©—è­‰é€²åº¦æ¢
        if self.use_progress_bar and self.val_pbar:
            self.val_pbar.close()
        
        avg_val_loss = val_loss / num_batches
        
        # è¨ˆç®—å¹³å‡ Dice åˆ†æ•¸ - è™•ç†åˆ—è¡¨æ ¼å¼
        if val_dice_scores:
            if isinstance(val_dice_scores[0], (list, tuple)):
                # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå–æ¯å€‹æ‰¹æ¬¡çš„ç¬¬ä¸€å€‹å…ƒç´ ï¼ˆå‰æ™¯é¡åˆ¥ï¼‰
                avg_val_dice = np.mean([score[0] if len(score) > 0 else 0.0 for score in val_dice_scores])
            else:
                # å¦‚æœæ˜¯å–®ä¸€æ•¸å€¼æ ¼å¼
                avg_val_dice = np.mean(val_dice_scores)
        else:
            avg_val_dice = 0.0
        
        if not self.use_progress_bar:
            print(f'\n  é©—è­‰å®Œæˆ | å¹³å‡æå¤±: {avg_val_loss:.4f} | å¹³å‡Dice: {avg_val_dice:.4f}')
        
        return avg_val_loss, avg_val_dice
    
    def visualize_epoch_results(self, epoch, sample_predictions=None):
        """è¦–è¦ºåŒ–ç•¶å‰epochçš„çµæœ"""
        if not self.visualize:
            return
        
        try:
            if epoch % self.plot_interval == 0:
                if self.use_progress_bar:
                    tqdm.write("æ›´æ–°è¨“ç·´æ›²ç·š...")
                else:
                    print("æ›´æ–°è¨“ç·´æ›²ç·š...")
                
                # æª¢æŸ¥visualizerçš„å¯ç”¨æ–¹æ³•
                if hasattr(self.visualizer, 'plot_training_curves'):
                    self.visualizer.plot_training_curves(
                        self.history,
                        title=f"Training curve (up to Epoch {epoch+1})",
                        save_name=f"training_curves_epoch_{epoch+1}.png"
                    )
                else:
                    print("è¦–è¦ºåŒ–å™¨æ²’æœ‰ plot_training_curves æ–¹æ³•")
            
            if sample_predictions is not None:
                images, masks, predictions = sample_predictions
                if hasattr(self.visualizer, 'plot_3d_predictions'):
                    self.visualizer.plot_3d_predictions(
                        images, masks, predictions,
                        save_name=f"predictions_epoch_{epoch+1}.png",
                        max_samples=4
                    )
                else:
                    print("è¦–è¦ºåŒ–å™¨æ²’æœ‰ plot_3d_predictions æ–¹æ³•")
                
        except Exception as e:
            if self.use_progress_bar:
                tqdm.write(f"è¦–è¦ºåŒ–éç¨‹å‡ºç¾éŒ¯èª¤: {e}")
            else:
                print(f"è¦–è¦ºåŒ–éç¨‹å‡ºç¾éŒ¯èª¤: {e}")
            # ä¸ä¸­æ–·è¨“ç·´ï¼Œç¹¼çºŒåŸ·è¡Œ
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æª¢æŸ¥é» - ä½¿ç”¨å¯¦éš›çš„è¨“ç·´é…ç½®è€Œéæ¨¡å‹é è¨­å€¼"""
        # å¾è¨“ç·´é…ç½®ä¸­æå–æ¨¡å‹é…ç½®ï¼ˆå„ªå…ˆä½¿ç”¨å¯¦éš›é…ç½®ï¼‰
        if self.training_config:
            model_config = {
                'n_channels': self.training_config.get('n_channels', getattr(self.model, 'n_channels', 1)),
                'n_classes': self.training_config.get('n_classes', getattr(self.model, 'n_classes', 2)),
                'base_channels': self.training_config.get('base_channels', getattr(self.model, 'base_channels', 64)),
                'num_groups': self.training_config.get('num_groups', getattr(self.model, 'num_groups', 8)),
                'bilinear': self.training_config.get('bilinear', getattr(self.model, 'bilinear', False))
            }
        else:
            # é™ç´šæ–¹æ¡ˆï¼šå¾æ¨¡å‹å‹•æ…‹ç²å–
            model_config = {
                'n_channels': getattr(self.model, 'n_channels', 1),
                'n_classes': getattr(self.model, 'n_classes', 2),
                'base_channels': getattr(self.model, 'base_channels', 64),
                'num_groups': getattr(self.model, 'num_groups', 8),
                'bilinear': getattr(self.model, 'bilinear', False)
            }
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.history['train_loss'],
            'val_losses': self.history['val_loss'],
            'train_dice': self.history['train_dice'],
            'val_dice': self.history['val_dice'],
            'learning_rate': self.history['learning_rate'],
            'best_val_dice': self.best_val_dice,
            'total_training_time': self.total_training_time,
            
            # ä½¿ç”¨å¯¦éš›çš„è¨“ç·´é…ç½®
            'model_config': model_config,
            
            # é¡å¤–ä¿å­˜å®Œæ•´çš„è¨“ç·´é…ç½®ï¼ˆå¯é¸ï¼‰
            'training_config': self.training_config,
            
            # æ¨¡å‹è¤‡é›œåº¦è³‡è¨Š
            'model_info': self.model_info
        }
        
        if self.scheduler is not None:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        # ä¿å­˜æª¢æŸ¥é»
        latest_path = self.save_dir / 'latest_checkpoint.pth'
        torch.save(checkpoint, latest_path)
        
        if is_best:
            best_path = self.save_dir / 'best_model.pth'
            torch.save(checkpoint, best_path)
            print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")
            
        # å®šæœŸé¡¯ç¤ºä¿å­˜è³‡è¨Š
        if (epoch + 1) % (self.save_interval * 2) == 0:
            print(f"æª¢æŸ¥é»å·²ä¿å­˜è‡³: {latest_path}")
    
    def load_checkpoint(self, checkpoint_path):
        """è¼‰å…¥æ¨¡å‹æª¢æŸ¥é» - ä¿®å¾© PyTorch 2.6 å…¼å®¹æ€§å•é¡Œ"""
        try:
            checkpoint = self.safe_torch_load(checkpoint_path)
        except Exception as e:
            print(f"è¼‰å…¥æª¢æŸ¥é»å¤±æ•—: {e}")
            raise
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.best_val_dice = checkpoint.get('best_val_dice', 0.0)
        self.history = checkpoint.get('history', self.history)
        
        if self.scheduler and 'scheduler_state_dict' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        start_epoch = checkpoint['epoch'] + 1
        print(f"è¼‰å…¥æª¢æŸ¥é»ï¼Œå¾ç¬¬ {start_epoch} epoch é–‹å§‹")
        print(f"æ­·å²æœ€ä½³Dice: {self.best_val_dice:.4f}")
        return start_epoch
    
    def get_sample_predictions(self):
        """ç²å–æ¨£æœ¬é æ¸¬çµæœç”¨æ–¼è¦–è¦ºåŒ–"""
        if 'val' not in self.data_loaders:
            return None
        
        self.model.eval()
        with torch.no_grad():
            for batch in self.data_loaders['val']:
                images = batch['image'].to(self.device)
                masks = batch['mask'].to(self.device)
                masks[masks > 0] = 1
                
                outputs = self.model(images)
                
                return (images[:4], masks[:4], outputs[:4])
        return None
    
    def train(self, num_epochs, resume_from=None, early_stopping_patience=None):
        """è¨“ç·´æ¨¡å‹ - ä¿®å¾© warmup å­¸ç¿’ç‡é¡¯ç¤ºå•é¡Œ"""
        print(f"é–‹å§‹è¨“ç·´ {num_epochs} epochs")
        
        # è¨˜éŒ„è¨“ç·´é–‹å§‹æ™‚é–“
        self.training_start_time = time.time()
        
        start_epoch = 0
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ warmup scheduler
        has_warmup = hasattr(self, 'warmup_scheduler') and self.warmup_scheduler is not None
        warmup_epochs = getattr(self, 'warmup_epochs', 0) if has_warmup else 0
        
        if has_warmup:
            print(f"ğŸ”¥ Warmup å·²å•Ÿç”¨: å‰ {warmup_epochs} epochs å°‡ä½¿ç”¨ warmup èª¿åº¦")
        
        # å¾æª¢æŸ¥é»æ¢å¾©
        if resume_from and Path(resume_from).exists():
            checkpoint = self.safe_torch_load(resume_from)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            
            # æ¢å¾©æ­·å²è¨˜éŒ„
            self.history['train_loss'] = checkpoint.get('train_losses', [])
            self.history['val_loss'] = checkpoint.get('val_losses', [])
            self.history['train_dice'] = checkpoint.get('train_dice', [])
            self.history['val_dice'] = checkpoint.get('val_dice', [])
            
            # æ¢å¾©å­¸ç¿’ç‡æ­·å²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'learning_rate' in checkpoint:
                self.history['learning_rate'] = checkpoint['learning_rate']
            else:
                # å¦‚æœæ²’æœ‰å­¸ç¿’ç‡æ­·å²ï¼Œç”¨ç•¶å‰å­¸ç¿’ç‡å¡«å……
                current_lr = self.optimizer.param_groups[0]['lr']
                self.history['learning_rate'] = [current_lr] * len(self.history['train_loss'])
            
            self.best_val_dice = checkpoint.get('best_val_dice', 0)
            
            # æ¢å¾©å·²è¨“ç·´æ™‚é–“
            self.total_training_time = checkpoint.get('total_training_time', 0)
            
            print(f"å¾ epoch {start_epoch} æ¢å¾©è¨“ç·´")
            print(f"å·²è¨“ç·´æ™‚é–“: {self.total_training_time/3600:.2f} å°æ™‚")
            print(f"ç•¶å‰å­¸ç¿’ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # å¦‚æœæœ‰ warmupï¼Œéœ€è¦èª¿æ•´ warmup scheduler çš„ç‹€æ…‹
            if has_warmup and start_epoch < warmup_epochs:
                self.warmup_scheduler.current_epoch = start_epoch
                print(f"ğŸ”¥ æ¢å¾© warmup ç‹€æ…‹: epoch {start_epoch}/{warmup_epochs}")
        
        # æ—©åœæ©Ÿåˆ¶
        early_stopping_counter = 0
        
        try:
            for epoch in range(start_epoch, num_epochs):
                epoch_start_time = time.time()
                
                # ==================== é—œéµä¿®æ”¹ï¼šåœ¨æ¯å€‹ epoch é–‹å§‹æ™‚èª¿ç”¨ warmup scheduler ====================
                if has_warmup:
                    if epoch < warmup_epochs:
                        # Warmup éšæ®µï¼šèª¿ç”¨ warmup scheduler
                        self.warmup_scheduler.step(epoch)
                        warmup_stage = True
                        
                        # ç²å–ç•¶å‰çš„å­¸ç¿’ç‡å’Œå‹•é‡ç”¨æ–¼é¡¯ç¤º
                        current_lrs = self.warmup_scheduler.get_lr()
                        current_momenta = self.warmup_scheduler.get_momentum()
                        
                        if self.use_progress_bar:
                            tqdm.write(f"ğŸ”¥ Warmup éšæ®µ [{epoch+1}/{warmup_epochs}] - LR: {current_lrs[0]:.6f}, Momentum: {current_momenta[0]:.3f}")
                        else:
                            print(f"ğŸ”¥ Warmup éšæ®µ [{epoch+1}/{warmup_epochs}] - LR: {current_lrs[0]:.6f}, Momentum: {current_momenta[0]:.3f}")
                    else:
                        warmup_stage = False
                else:
                    warmup_stage = False
                
                # è¨“ç·´ä¸€å€‹epoch
                train_loss, train_dice = self.train_one_epoch(epoch)
                val_loss, val_dice = self.validate_one_epoch(epoch)
                
                # è¨˜éŒ„æ­·å²
                self.history['train_loss'].append(train_loss)
                self.history['val_loss'].append(val_loss)
                self.history['train_dice'].append(train_dice)
                self.history['val_dice'].append(val_dice)
                
                # è¨˜éŒ„å­¸ç¿’ç‡ï¼ˆåœ¨ warmup æˆ–ä¸»èª¿åº¦å™¨ä¹‹å¾Œï¼‰
                current_lr = self.optimizer.param_groups[0]['lr']
                self.history['learning_rate'].append(current_lr)
                
                epoch_time = time.time() - epoch_start_time
                self.total_training_time += epoch_time
                
                # ==================== å­¸ç¿’ç‡èª¿åº¦ ====================
                if not warmup_stage and self.scheduler:
                    # åªæœ‰åœ¨é warmup éšæ®µæ‰èª¿ç”¨ä¸»èª¿åº¦å™¨
                    if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                        self.scheduler.step(val_dice)
                    else:
                        self.scheduler.step()
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                is_best = val_dice > self.best_val_dice
                if is_best:
                    self.best_val_dice = val_dice
                    early_stopping_counter = 0
                else:
                    early_stopping_counter += 1
                
                # æ—¥èªŒè¼¸å‡º - å¢å¼·ç‰ˆï¼šé¡¯ç¤ºç•¶å‰éšæ®µå’Œå‹•é‡è³‡è¨Š
                if (epoch + 1) % self.log_interval == 0 or is_best:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    
                    # ç²å–å‹•é‡è³‡è¨Š
                    momentum_info = ""
                    if 'momentum' in self.optimizer.param_groups[0]:
                        momentum = self.optimizer.param_groups[0]['momentum']
                        momentum_info = f", Momentum: {momentum:.3f}"
                    elif 'betas' in self.optimizer.param_groups[0]:
                        beta1 = self.optimizer.param_groups[0]['betas'][0]
                        momentum_info = f", Beta1: {beta1:.3f}"
                    
                    # éšæ®µæ¨™è¨˜
                    stage_info = "ğŸ”¥ [WARMUP]" if warmup_stage else "ğŸš€ [NORMAL]"
                    
                    log_msg = (f"{stage_info} Epoch [{epoch+1}/{num_epochs}] "
                            f"Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f} | "
                            f"Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f} | "
                            f"LR: {current_lr:.6f}{momentum_info} | Time: {epoch_time:.1f}s")
                    
                    if is_best:
                        log_msg += " â­ [NEW BEST]"
                    
                    if self.use_progress_bar:
                        tqdm.write(log_msg)
                    else:
                        print(log_msg)
                
                # ä¿å­˜æª¢æŸ¥é»
                if (epoch + 1) % self.save_interval == 0 or is_best:
                    self.save_checkpoint(epoch, is_best)
                
                # è¦–è¦ºåŒ– - åŒ…å«é æ¸¬çµæœ
                if self.visualize and (epoch + 1) % self.plot_interval == 0:
                    try:
                        # ç¹ªè£½è¨“ç·´æ›²ç·š
                        self.visualizer.plot_training_curves(self.history, title=f"Training curve (up to Epoch {epoch+1})" , save_name=f"training_curves_epoch_{epoch+1:03d}.png")
                        
                        # ç²å–ä¸¦è¦–è¦ºåŒ–é æ¸¬çµæœ
                        sample_predictions = self.get_sample_predictions()
                        if sample_predictions is not None:
                            images, masks, predictions = sample_predictions
                            if hasattr(self.visualizer, 'plot_3d_predictions'):
                                self.visualizer.plot_3d_predictions(
                                    images, masks, predictions,
                                    save_name=f"predictions_epoch_{epoch+1:03d}.png",
                                    max_samples=3
                                )
                                if self.use_progress_bar:
                                    tqdm.write(f"å·²ç”Ÿæˆç¬¬ {epoch+1} epoch çš„é æ¸¬çµæœåœ–")
                                else:
                                    print(f"å·²ç”Ÿæˆç¬¬ {epoch+1} epoch çš„é æ¸¬çµæœåœ–")
                    except Exception as e:
                        error_msg = f"è¦–è¦ºåŒ–éç¨‹å‡ºç¾éŒ¯èª¤: {e}"
                        if self.use_progress_bar:
                            tqdm.write(error_msg)
                        else:
                            print(error_msg)
                
                # æ—©åœæª¢æŸ¥
                if early_stopping_patience and early_stopping_counter >= early_stopping_patience:
                    print(f"æ—©åœè§¸ç™¼ï¼å·²é€£çºŒ {early_stopping_patience} epochs ç„¡æ”¹å–„")
                    break
            
            # è¨˜éŒ„è¨“ç·´çµæŸæ™‚é–“
            self.training_end_time = time.time()
            
            print(f"è¨“ç·´å®Œæˆï¼")
            print(f"ç¸½è¨“ç·´æ™‚é–“: {self.total_training_time/3600:.2f} å°æ™‚")
            print(f"æœ€ä½³é©—è­‰ Dice: {self.best_val_dice:.4f}")
            
            # æœ€çµ‚è¦–è¦ºåŒ–
            if self.visualize:
                try:
                    # æ ¹æ“švisualizerçš„å¯¦éš›æ–¹æ³•ä¾†èª¿ç”¨
                    if hasattr(self.visualizer, 'create_final_dashboard'):
                        self.visualizer.create_final_dashboard(self.history)
                    elif hasattr(self.visualizer, 'create_training_dashboard'):
                        self.visualizer.create_training_dashboard(self.history, "final_training_dashboard.png")
                    else:
                        # é€€å›åˆ°åŸºæœ¬çš„è¨“ç·´æ›²ç·šç¹ªè£½
                        self.visualizer.plot_training_curves(self.history, len(self.history['train_loss']))
                except Exception as e:
                    print(f"æœ€çµ‚è¦–è¦ºåŒ–ç”Ÿæˆå¤±æ•—: {e}")
                    print("è·³éè¦–è¦ºåŒ–æ­¥é©Ÿ")
        
        except KeyboardInterrupt:
            print("è¨“ç·´è¢«ä¸­æ–·")
            self.training_end_time = time.time()
        
        return self.history

    
    def test(self, checkpoint_path=None, save_results=True):
        """æ¸¬è©¦æ¨¡å‹ä¸¦ç”Ÿæˆè¦–è¦ºåŒ–çµæœï¼ŒåŒæ™‚ä¿å­˜æ¸¬è©¦å ±å‘Š"""
        if 'test' not in self.data_loaders:
            print("æ²’æœ‰æ¸¬è©¦è³‡æ–™é›†")
            return None
        
        if checkpoint_path:
            try:
                checkpoint = self.safe_torch_load(checkpoint_path)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print(f"è¼‰å…¥æ¸¬è©¦æ¨¡å‹: {checkpoint_path}")
            except Exception as e:
                print(f"è¼‰å…¥æ¸¬è©¦æ¨¡å‹å¤±æ•—: {e}")
                return None
        
        self.model.eval()
        test_dice_scores = []
        test_losses = []
        all_predictions = []
        test_details = []  # è¨˜éŒ„è©³ç´°è³‡è¨Š
        
        num_batches = len(self.data_loaders['test'])
        
        # å‰µå»ºæ¸¬è©¦é€²åº¦æ¢
        if self.use_progress_bar:
            test_pbar = tqdm(
                enumerate(self.data_loaders['test']),
                total=num_batches,
                desc="æ¸¬è©¦æ¨¡å‹",
                ncols=100,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {postfix}'
            )
        else:
            print("é–‹å§‹æ¸¬è©¦...")
        
        data_iter = test_pbar if self.use_progress_bar else enumerate(self.data_loaders['test'])
        
        with torch.no_grad():
            for batch_idx, batch in data_iter:
                images = batch['image'].to(self.device)
                masks = batch['mask'].to(self.device)
                masks[masks > 0] = 1
                
                outputs = self.model(images)

                # è¨ˆç®—æ‰¹æ¬¡æå¤±ï¼ˆç”¨æ–¼é€²åº¦é¡¯ç¤ºï¼‰
                batch_loss = self.criterion(outputs, masks)
                test_losses.append(batch_loss.item())

                # è¨ˆç®—æ‰¹æ¬¡Diceåˆ†æ•¸ï¼ˆç”¨æ–¼é€²åº¦é¡¯ç¤ºï¼‰
                batch_metrics = calculate_metrics(outputs, masks, self.model.n_classes)
                batch_dice_score = batch_metrics['mean_dice']
                test_dice_scores.append(batch_dice_score)

                # ç‚ºæ¯å€‹æ¨£æœ¬è¨ˆç®—å€‹åˆ¥çš„æå¤±å’ŒDiceåˆ†æ•¸
                batch_size = images.size(0)
                individual_losses = []
                individual_dice_scores = []

                for sample_idx in range(batch_size):
                    # æå–å–®å€‹æ¨£æœ¬
                    single_image = images[sample_idx:sample_idx+1]
                    single_mask = masks[sample_idx:sample_idx+1]
                    single_output = outputs[sample_idx:sample_idx+1]
                    
                    # è¨ˆç®—å–®å€‹æ¨£æœ¬çš„æå¤±
                    single_loss = self.criterion(single_output, single_mask)
                    individual_losses.append(single_loss.item())
                    
                    # è¨ˆç®—å–®å€‹æ¨£æœ¬çš„Diceåˆ†æ•¸
                    single_metrics = calculate_metrics(single_output, single_mask, self.model.n_classes)
                    single_dice = single_metrics['mean_dice']
                    individual_dice_scores.append(single_dice)

                # è¨˜éŒ„è©³ç´°è³‡è¨Šï¼ˆæ¯å€‹æª”æ¡ˆï¼‰
                if 'image_path' in batch:
                    for i, img_path in enumerate(batch['image_path']):
                        if i < len(individual_losses):
                            test_details.append({
                                'file': Path(img_path).name,
                                'loss': individual_losses[i],
                                'dice': individual_dice_scores[i],
                                'batch_idx': batch_idx,
                                'sample_idx': i
                            })
                else:
                    # å¦‚æœæ²’æœ‰æª”æ¡ˆè·¯å¾‘ï¼Œè¨˜éŒ„æ¯å€‹æ¨£æœ¬çš„è³‡è¨Š
                    for i in range(len(individual_losses)):
                        test_details.append({
                            'file': f'batch_{batch_idx:03d}_sample_{i:02d}',
                            'loss': individual_losses[i],
                            'dice': individual_dice_scores[i],
                            'batch_idx': batch_idx,
                            'sample_idx': i
                        })
                
                # æ”¶é›†é æ¸¬çµæœç”¨æ–¼è¦–è¦ºåŒ–
                if batch_idx < 3 and self.visualize:
                    all_predictions.append((images.cpu(), masks.cpu(), outputs.cpu()))
                
                # æ›´æ–°é€²åº¦æ¢
                if self.use_progress_bar:
                    test_pbar.set_postfix({
                        'Loss': f'{batch_loss.item():.4f}',
                        'Dice': f'{batch_dice_score:.4f}'
                    })
                elif batch_idx % 5 == 0:
                    print(f"æ¸¬è©¦é€²åº¦: {batch_idx+1}/{num_batches}")
        
        # é—œé–‰æ¸¬è©¦é€²åº¦æ¢
        if self.use_progress_bar and 'test_pbar' in locals():
            test_pbar.close()
        
        # ============== é‡é»ä¿®å¾©ï¼šåŸºæ–¼å€‹åˆ¥æª”æ¡ˆè¨ˆç®—çœŸå¯¦çµ±è¨ˆ ==============
        if test_details:
            # ä½¿ç”¨å€‹åˆ¥æª”æ¡ˆçš„çœŸå¯¦æ•¸æ“š
            individual_losses = [detail['loss'] for detail in test_details]
            individual_dice_scores = [detail['dice'] for detail in test_details]
            
            avg_test_loss = np.mean(individual_losses)
            avg_test_dice = np.mean(individual_dice_scores)
            
            # è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
            loss_std = np.std(individual_losses)
            dice_std = np.std(individual_dice_scores)
            min_loss = np.min(individual_losses)
            max_loss = np.max(individual_losses)
            min_dice = np.min(individual_dice_scores)
            max_dice = np.max(individual_dice_scores)
            
            # ç”¨æ–¼è¿”å›å’Œä¿å­˜çš„æ•¸æ“š
            final_losses = individual_losses
            final_dice_scores = individual_dice_scores
            sample_count = len(test_details)
            unit_description = "å€‹æª”æ¡ˆ"
        else:
            # é™ç´šåˆ°æ‰¹æ¬¡æ•¸æ“š
            avg_test_loss = np.mean(test_losses)
            avg_test_dice = np.mean(test_dice_scores)
            
            loss_std = np.std(test_losses)
            dice_std = np.std(test_dice_scores)
            min_loss = np.min(test_losses)
            max_loss = np.max(test_losses)
            min_dice = np.min(test_dice_scores)
            max_dice = np.max(test_dice_scores)
            
            final_losses = test_losses
            final_dice_scores = test_dice_scores
            sample_count = len(test_losses)
            unit_description = "å€‹æ‰¹æ¬¡"
        
        result_msg = f"""
        æ¸¬è©¦çµæœ:
        æ¸¬è©¦æ¨£æœ¬æ•¸: {sample_count} {unit_description}
        å¹³å‡æå¤±: {avg_test_loss:.4f}
        å¹³å‡ Dice åˆ†æ•¸: {avg_test_dice:.4f} ({avg_test_dice * 100:.2f}%)
        æå¤±ç¯„åœ: {min_loss:.4f} ~ {max_loss:.4f} (æ¨™æº–å·®: {loss_std:.4f})
        Diceç¯„åœ: {min_dice:.4f} ~ {max_dice:.4f} (æ¨™æº–å·®: {dice_std:.4f})"""
        
        if self.use_progress_bar:
            tqdm.write(result_msg)
        else:
            print(result_msg)
        
        # ä¿å­˜æ¸¬è©¦çµæœåˆ°æ–‡ä»¶
        if save_results:
            self._save_test_results(
                avg_test_loss, avg_test_dice,
                final_losses, final_dice_scores,
                test_details, checkpoint_path,
                # å‚³éçµ±è¨ˆæŒ‡æ¨™
                loss_std, dice_std, min_loss, max_loss, min_dice, max_dice,
                sample_count, unit_description
            )
        
        # ç”Ÿæˆæ¸¬è©¦è¦–è¦ºåŒ–
        if self.visualize and all_predictions:
            viz_msg = "ç”Ÿæˆæ¸¬è©¦çµæœè¦–è¦ºåŒ–..."
            if self.use_progress_bar:
                tqdm.write(viz_msg)
            else:
                print(viz_msg)
            
            try:
                for i, (images, masks, predictions) in enumerate(all_predictions):
                    if hasattr(self.visualizer, 'plot_3d_predictions'):
                        self.visualizer.plot_3d_predictions(
                            images, masks, predictions,
                            save_name=f"test_results_batch_{i+1}.png"
                        )
                    else:
                        print("è¦–è¦ºåŒ–å™¨æ²’æœ‰ plot_3d_predictions æ–¹æ³•ï¼Œè·³éæ¸¬è©¦çµæœè¦–è¦ºåŒ–")
                        break
            except Exception as e:
                error_msg = f"æ¸¬è©¦çµæœè¦–è¦ºåŒ–å¤±æ•—: {e}"
                if self.use_progress_bar:
                    tqdm.write(error_msg)
                else:
                    print(error_msg)
        
        return {
            'avg_loss': avg_test_loss,
            'avg_dice': avg_test_dice,
            'all_losses': final_losses,
            'all_dice_scores': final_dice_scores,
            'details': test_details,
            'stats': {
                'loss_std': loss_std,
                'dice_std': dice_std,
                'min_loss': min_loss,
                'max_loss': max_loss,
                'min_dice': min_dice,
                'max_dice': max_dice,
                'sample_count': sample_count,
                'unit': unit_description
            }
        }

    def _save_test_results(self, avg_loss, avg_dice, all_losses, all_dice, details, model_path,
                        loss_std, dice_std, min_loss, max_loss, min_dice, max_dice,
                        sample_count, unit_description):
        """å¢å¼·ç‰ˆæ¸¬è©¦çµæœä¿å­˜ - ä½¿ç”¨å¯¦éš›çš„è¨“ç·´é…ç½®"""
        import datetime
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å¾æª¢æŸ¥é»è¼‰å…¥é¡å¤–è³‡è¨Š
        training_info = {}
        model_config = {}
        model_complexity = {}
        full_training_config = {}  # æ–°å¢ï¼šå®Œæ•´è¨“ç·´é…ç½®
        
        if model_path and Path(model_path).exists():
            try:
                checkpoint = self.safe_torch_load(model_path)
                training_info = {
                    'total_epochs': checkpoint.get('epoch', 'Unknown') + 1,
                    'best_val_dice': checkpoint.get('best_val_dice', 'Unknown'),
                    'total_training_time': checkpoint.get('total_training_time', 0),
                    'final_train_loss': checkpoint.get('train_losses', [])[-1] if checkpoint.get('train_losses') else 'Unknown',
                    'final_val_loss': checkpoint.get('val_losses', [])[-1] if checkpoint.get('val_losses') else 'Unknown'
                }
                # å„ªå…ˆä½¿ç”¨æª¢æŸ¥é»ä¸­çš„å¯¦éš›é…ç½®
                model_config = checkpoint.get('model_config', {})
                full_training_config = checkpoint.get('training_config', {})
                model_complexity = checkpoint.get('model_info', self.model_info)
            except:
                pass
        
        # å¦‚æœæª¢æŸ¥é»ä¸­æ²’æœ‰é…ç½®ï¼Œä½¿ç”¨ç•¶å‰ trainer çš„é…ç½®
        if not model_config and self.training_config:
            model_config = {
                'n_channels': self.training_config.get('n_channels', 1),
                'n_classes': self.training_config.get('n_classes', 2),
                'base_channels': self.training_config.get('base_channels', 32),
                'num_groups': self.training_config.get('num_groups', 8),
                'bilinear': self.training_config.get('bilinear', False)
            }
        
        # ä¿å­˜åˆ°txtæ–‡ä»¶
        txt_file = self.save_dir / f"test_results_{timestamp}.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("3D UNet æ¨¡å‹å®Œæ•´æ¸¬è©¦å ±å‘Š\n")
            f.write("=" * 70 + "\n")
            f.write(f"æ¸¬è©¦æ™‚é–“: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ¨¡å‹è·¯å¾‘: {model_path or 'ç•¶å‰è¨“ç·´æ¨¡å‹'}\n")
            f.write("\n")
            
            # æ¨¡å‹åŸºæœ¬è³‡è¨Šï¼ˆä½¿ç”¨å¯¦éš›é…ç½®ï¼‰
            f.write("=" * 25 + " æ¨¡å‹åŸºæœ¬è³‡è¨Š " + "=" * 25 + "\n")
            f.write(f"è¼¸å…¥é€šé“æ•¸ (n_channels): {model_config.get('n_channels', 'Unknown')}\n")
            f.write(f"è¼¸å‡ºé¡åˆ¥æ•¸ (n_classes): {model_config.get('n_classes', 'Unknown')}\n")
            f.write(f"åŸºç¤é€šé“æ•¸ (base_channels): {model_config.get('base_channels', 'Unknown')}\n")
            f.write(f"GroupNorm çµ„æ•¸ (num_groups): {model_config.get('num_groups', 'Unknown')}\n")
            f.write(f"é›™ç·šæ€§ä¸Šæ¡æ¨£ (bilinear): {model_config.get('bilinear', 'Unknown')}\n")
            f.write("\n")
            
            # å¦‚æœæœ‰å®Œæ•´çš„è¨“ç·´é…ç½®ï¼Œé¡å¤–é¡¯ç¤ºé—œéµè¨“ç·´åƒæ•¸
            if full_training_config:
                f.write("=" * 24 + " è¨“ç·´é…ç½®åƒæ•¸ " + "=" * 24 + "\n")
                f.write(f"æ‰¹æ¬¡å¤§å° (batch_size): {full_training_config.get('batch_size', 'Unknown')}\n")
                f.write(f"å­¸ç¿’ç‡ (learning_rate): {full_training_config.get('learning_rate', 'Unknown')}\n")
                f.write(f"å„ªåŒ–å™¨ (optimizer): {full_training_config.get('optimizer', 'Unknown')}\n")
                f.write(f"æå¤±å‡½æ•¸ (loss_type): {full_training_config.get('loss_type', 'Unknown')}\n")
                f.write(f"æ•¸æ“šå¢å¼· (use_augmentation): {full_training_config.get('use_augmentation', 'Unknown')}\n")
                if full_training_config.get('use_augmentation'):
                    f.write(f"å¢å¼·é¡å‹ (augmentation_type): {full_training_config.get('augmentation_type', 'Unknown')}\n")
                f.write(f"å½±åƒå°ºå¯¸ (target_size): {full_training_config.get('target_size', 'Unknown')}\n")
                f.write("\n")
            
            # æ¨¡å‹è¤‡é›œåº¦è³‡è¨Š
            f.write("=" * 25 + " æ¨¡å‹è¤‡é›œåº¦è³‡è¨Š " + "=" * 24 + "\n")
            f.write(f"ç¸½åƒæ•¸é‡: {model_complexity.get('total_params', 'Unknown'):,}\n")
            f.write(f"å¯è¨“ç·´åƒæ•¸: {model_complexity.get('trainable_params', 'Unknown'):,}\n")
            f.write(f"æ¨¡å‹å¤§å°: {model_complexity.get('model_size_mb', 0):.2f} MB (FP32)\n")
            f.write(f"GLOPs: {model_complexity.get('flops', 'Unknown')}\n")
            f.write(f"å¹³å‡æ¨ç†æ™‚é–“: {model_complexity.get('avg_inference_time', 0)*1000:.2f} ms\n")
            f.write("\n")
            
            # è¨“ç·´è³‡è¨Š
            f.write("=" * 26 + " è¨“ç·´è³‡è¨Š " + "=" * 26 + "\n")
            if training_info:
                f.write(f"è¨“ç·´è¼ªæ•¸: {training_info.get('total_epochs', 'Unknown')} epochs\n")
                total_time = training_info.get('total_training_time', 0)
                if total_time > 0:
                    hours = int(total_time // 3600)
                    minutes = int((total_time % 3600) // 60)
                    seconds = int(total_time % 60)
                    f.write(f"ç¸½è¨“ç·´æ™‚é–“: {hours:02d}:{minutes:02d}:{seconds:02d} ({total_time/3600:.2f} å°æ™‚)\n")
                    f.write(f"å¹³å‡æ¯epochæ™‚é–“: {total_time/training_info.get('total_epochs', 1):.1f} ç§’\n")
                f.write(f"è¨“ç·´é›†æœ€çµ‚æå¤±: {training_info.get('final_train_loss', 'Unknown')}\n")
                f.write(f"é©—è­‰é›†æœ€çµ‚æå¤±: {training_info.get('final_val_loss', 'Unknown')}\n")
                f.write(f"æœ€ä½³é©—è­‰Dice: {training_info.get('best_val_dice', 'Unknown')}\n")
            else:
                f.write("ç„¡æ³•è¼‰å…¥è¨“ç·´è³‡è¨Š\n")
            f.write("\n")
            
            # æ¸¬è©¦çµæœ
            f.write("=" * 27 + " æ¸¬è©¦çµæœ " + "=" * 27 + "\n")
            f.write(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {sample_count} {unit_description}\n")
            f.write(f"å¹³å‡æ¸¬è©¦æå¤±: {avg_loss:.6f}\n")
            f.write(f"å¹³å‡æ¸¬è©¦Dice: {avg_dice:.6f} ({avg_dice * 100:.2f}%)\n")
            f.write("\n")
            
            # çµ±è¨ˆåˆ†æ
            f.write("=" * 27 + " çµ±è¨ˆåˆ†æ " + "=" * 27 + "\n")
            f.write(f"æå¤± - æ¨™æº–å·®: {loss_std:.6f}\n")
            f.write(f"æå¤± - æœ€å°å€¼: {min_loss:.6f}\n")
            f.write(f"æå¤± - æœ€å¤§å€¼: {max_loss:.6f}\n")
            f.write(f"Dice - æ¨™æº–å·®: {dice_std:.6f}\n")
            f.write(f"Dice - æœ€å°å€¼: {min_dice:.6f}\n")
            f.write(f"Dice - æœ€å¤§å€¼: {max_dice:.6f}\n")
            f.write("\n")
            
            # æ€§èƒ½è©•ä¼°
            f.write("=" * 27 + " æ€§èƒ½è©•ä¼° " + "=" * 27 + "\n")
            if avg_dice >= 0.90:
                performance = "å„ªç§€"
                evaluation = "æ¨¡å‹è¡¨ç¾å‡ºè‰²ï¼Œå¯ä»¥ç”¨æ–¼å¯¦éš›æ‡‰ç”¨"
            elif avg_dice >= 0.80:
                performance = "è‰¯å¥½" 
                evaluation = "æ¨¡å‹è¡¨ç¾è‰¯å¥½ï¼Œå¯è€ƒæ…®é€²ä¸€æ­¥å„ªåŒ–"
            elif avg_dice >= 0.75:
                performance = "ä¸­ç­‰"
                evaluation = "æ¨¡å‹è¡¨ç¾ä¸­ç­‰ï¼Œå»ºè­°èª¿æ•´è¶…åƒæ•¸æˆ–å¢åŠ æ•¸æ“š"
            else:
                performance = "éœ€è¦æ”¹é€²"
                evaluation = "æ¨¡å‹è¡¨ç¾ä¸ä½³ï¼Œå»ºè­°æª¢æŸ¥æ•¸æ“šè³ªé‡å’Œæ¨¡å‹æ¶æ§‹"
                
            f.write(f"æ•´é«”è©•ç´š: {performance} (Dice: {avg_dice:.4f})\n")
            f.write(f"è©•ä¼°å»ºè­°: {evaluation}\n")
            f.write("\n")
            
            # è©³ç´°çµæœï¼ˆå¦‚æœä¸å¤ªå¤šï¼‰
            if details and len(details) <= 30:
                f.write("=" * 25 + " è©³ç´°æ¸¬è©¦çµæœ " + "=" * 25 + "\n")
                f.write(f"{'æª”æ¡ˆ/æ‰¹æ¬¡':<35} {'æå¤±':<12} {'Diceåˆ†æ•¸':<12}\n")
                f.write("-" * 65 + "\n")
                for detail in details:
                    f.write(f"{detail['file']:<35} {detail['loss']:<12.6f} {detail['dice']:<12.6f}\n")
            
            f.write("\n" + "=" * 70 + "\n")
            f.write("å ±å‘ŠçµæŸ\n")
        
        print(f"å®Œæ•´æ¸¬è©¦å ±å‘Šå·²ä¿å­˜åˆ°: {txt_file}")
        
        # JSONæ ¼å¼ä¿å­˜ï¼ˆåŒ…å«æ‰€æœ‰æ•¸æ“šï¼‰
        json_file = self.save_dir / f"test_results_{timestamp}.json"
        results_data = {
            'timestamp': datetime.datetime.now().isoformat(),
            'model_path': str(model_path) if model_path else 'current_model',
            'model_config': model_config,
            'training_config': full_training_config,  # æ–°å¢ï¼šå®Œæ•´è¨“ç·´é…ç½®
            'model_complexity': model_complexity,
            'training_info': training_info,
            'test_summary': {
                'avg_loss': float(avg_loss),
                'avg_dice': float(avg_dice),
                'sample_count': sample_count,
                'unit': unit_description,
                'loss_std': float(loss_std),
                'dice_std': float(dice_std),
                'best_dice': float(max_dice),
                'worst_dice': float(min_dice),
                'min_loss': float(min_loss),
                'max_loss': float(max_loss)
            },
            'all_losses': [float(x) for x in all_losses],
            'all_dice_scores': [float(x) for x in all_dice],
            'details': details
        }
        
        import json
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
        
        print(f"å®Œæ•´JSONæ•¸æ“šå·²ä¿å­˜åˆ°: {json_file}")